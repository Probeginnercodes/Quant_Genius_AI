{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef560d5-efec-460c-b91d-f714a6601655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸš€ VAST.AI ONE-CELL SOLUTION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ PART 1: Installing dependencies...\n",
      "Installing Python packages...\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling PyTorch with CUDA...\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… Dependencies installed!\n",
      "\n",
      "ðŸ“‚ PART 2: Finding and extracting data...\n",
      "ðŸ“¦ Found: ./quantgenius_complete_20260116_235212.zip\n",
      "ðŸ“‚ Extracting to: /workspace/quantgenius_project\n",
      "âœ… Extraction complete!\n",
      "\n",
      "ðŸ“Š PART 3: Loading all data...\n",
      "âš ï¸ Using default horizons\n",
      "\n",
      "ðŸ§© Loading DATASETS...\n",
      "âŒ No DATASETS found! Checking for parquet files...\n",
      "\n",
      "ðŸ’¾ Loading DataFrames...\n",
      "\n",
      "ðŸ”§ PART 4: Setting up globals for Cells 7-10...\n",
      "ðŸ–¥ï¸ Device: cuda\n",
      "âœ… GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "\n",
      "ðŸ” PART 5: Verification...\n",
      "âœ… DATASETS loaded: 0 horizons\n",
      "âœ… FORECAST_HORIZONS: {'short_1d': 1, 'short_3d': 3, 'short_5d': 5, 'intermediate_10d': 10, 'intermediate_15d': 15, 'intermediate_20d': 20}\n",
      "âœ… PROJECT_DIRS set up: 6 directories\n",
      "âœ… DEVICE: cuda\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ READY FOR CELLS 7-10!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ You can now copy-paste your Cells 7, 8, 9, 10 from Colab.\n",
      "ðŸ”§ Required globals are available:\n",
      "   â€¢ DATASETS\n",
      "   â€¢ PROJECT_DIRS\n",
      "   â€¢ FORECAST_HORIZONS\n",
      "   â€¢ ASSET_UNIVERSE, STOCKS, FX_PAIRS\n",
      "   â€¢ DEVICE (GPU ready)\n",
      "   â€¢ RANDOM_SEED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# ðŸš€ VAST.AI ONE-CELL SOLUTION (Everything in One Cell)\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸš€ VAST.AI ONE-CELL SOLUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ----- PART 1: INSTALL DEPENDENCIES -----\n",
    "print(\"\\nðŸ“¦ PART 1: Installing dependencies...\")\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install all required packages\n",
    "packages = [\n",
    "    \"numpy\", \"pandas\", \"matplotlib\", \"scipy\", \"scikit-learn\",\n",
    "    \"yfinance==0.2.36\", \"xgboost\", \"shap\", \"pyarrow\", \"fastparquet\"\n",
    "]\n",
    "\n",
    "print(\"Installing Python packages...\")\n",
    "for pkg in packages:\n",
    "    !pip -q install {pkg}\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "print(\"Installing PyTorch with CUDA...\")\n",
    "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")\n",
    "\n",
    "# ----- PART 2: FIND AND EXTRACT DATA -----\n",
    "print(\"\\nðŸ“‚ PART 2: Finding and extracting data...\")\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Look for the zip file\n",
    "zip_files = []\n",
    "search_paths = [\n",
    "    \"/home/user/\",  # Default upload location\n",
    "    \"/workspace/\",  # Persistent storage\n",
    "    \"/content/\",    # Colab-like\n",
    "    os.path.expanduser(\"~\"),  # Home directory\n",
    "    \".\"  # Current directory\n",
    "]\n",
    "\n",
    "for path in search_paths:\n",
    "    if os.path.exists(path):\n",
    "        found = glob.glob(os.path.join(path, \"quantgenius_complete_*.zip\"))\n",
    "        zip_files.extend(found)\n",
    "\n",
    "if not zip_files:\n",
    "    # Try any zip file\n",
    "    for path in search_paths:\n",
    "        if os.path.exists(path):\n",
    "            found = glob.glob(os.path.join(path, \"*.zip\"))\n",
    "            zip_files.extend(found)\n",
    "\n",
    "if zip_files:\n",
    "    zip_file = sorted(zip_files)[-1]  # Get most recent\n",
    "    print(f\"ðŸ“¦ Found: {zip_file}\")\n",
    "    \n",
    "    # Extract to workspace (persistent storage)\n",
    "    extract_to = \"/workspace/quantgenius_project\"\n",
    "    print(f\"ðŸ“‚ Extracting to: {extract_to}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"/workspace/\")\n",
    "    \n",
    "    print(\"âœ… Extraction complete!\")\n",
    "    BASE_DIR = Path(extract_to)\n",
    "else:\n",
    "    print(\"âš ï¸ No zip file found - checking for existing project...\")\n",
    "    # Check if project already exists\n",
    "    possible_locations = [\n",
    "        \"/workspace/quantgenius_project\",\n",
    "        \"/content/quantgenius_project\",\n",
    "        os.path.expanduser(\"~/quantgenius_project\"),\n",
    "        \"quantgenius_project\"\n",
    "    ]\n",
    "    \n",
    "    for loc in possible_locations:\n",
    "        if os.path.exists(loc):\n",
    "            BASE_DIR = Path(loc)\n",
    "            print(f\"âœ… Found existing project at: {loc}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"âŒ No project found! Please upload the zip file first.\")\n",
    "\n",
    "# ----- PART 3: LOAD ALL DATA -----\n",
    "print(\"\\nðŸ“Š PART 3: Loading all data...\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Set up project directories\n",
    "PROJECT_DIRS = {\n",
    "    \"data\": BASE_DIR / \"data\",\n",
    "    \"figures\": BASE_DIR / \"figures\",\n",
    "    \"tables\": BASE_DIR / \"tables\",\n",
    "    \"models\": BASE_DIR / \"models\",\n",
    "    \"reports\": BASE_DIR / \"reports\",\n",
    "    \"cache\": BASE_DIR / \"cache\"\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for name, path in PROJECT_DIRS.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "metadata_files = list((BASE_DIR / \"tables\").glob(\"targets_metadata_*.json\"))\n",
    "if metadata_files:\n",
    "    with open(sorted(metadata_files)[-1], 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    FORECAST_HORIZONS = metadata.get(\"forecast_horizons\", {\n",
    "        \"short_1d\": 1, \"short_3d\": 3, \"short_5d\": 5,\n",
    "        \"intermediate_10d\": 10, \"intermediate_15d\": 15, \"intermediate_20d\": 20\n",
    "    })\n",
    "    TEMPORAL_SPLITS = metadata.get(\"temporal_splits\", {})\n",
    "    print(f\"âœ… Loaded metadata: {len(FORECAST_HORIZONS)} horizons\")\n",
    "else:\n",
    "    print(\"âš ï¸ Using default horizons\")\n",
    "    FORECAST_HORIZONS = {\n",
    "        \"short_1d\": 1, \"short_3d\": 3, \"short_5d\": 5,\n",
    "        \"intermediate_10d\": 10, \"intermediate_15d\": 15, \"intermediate_20d\": 20\n",
    "    }\n",
    "    TEMPORAL_SPLITS = {}\n",
    "    metadata = {}\n",
    "\n",
    "MAX_HORIZON = max(FORECAST_HORIZONS.values())\n",
    "\n",
    "# Load DATASETS (numpy arrays)\n",
    "print(\"\\nðŸ§© Loading DATASETS...\")\n",
    "DATASETS = {}\n",
    "\n",
    "# Find the latest datasets folder\n",
    "datasets_folders = list((BASE_DIR / \"data\").glob(\"datasets_*\"))\n",
    "if datasets_folders:\n",
    "    latest_dataset = sorted(datasets_folders)[-1]\n",
    "    print(f\"ðŸ“‚ Loading from: {latest_dataset.name}\")\n",
    "    \n",
    "    for horizon in FORECAST_HORIZONS.keys():\n",
    "        horizon_dir = latest_dataset / horizon\n",
    "        if horizon_dir.exists():\n",
    "            DATASETS[horizon] = {}\n",
    "            npy_files = list(horizon_dir.glob(\"*.npy\"))\n",
    "            \n",
    "            for npy_file in npy_files:\n",
    "                try:\n",
    "                    array = np.load(npy_file)\n",
    "                    DATASETS[horizon][npy_file.stem] = array\n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ Failed to load {npy_file.name}: {e}\")\n",
    "            \n",
    "            # Print key shapes for verification\n",
    "            if npy_files:\n",
    "                print(f\"  âœ“ {horizon}: {len(npy_files)} arrays\")\n",
    "                if \"X_train\" in DATASETS[horizon]:\n",
    "                    print(f\"    X_train: {DATASETS[horizon]['X_train'].shape}\")\n",
    "                if \"y_train_raw\" in DATASETS[horizon]:\n",
    "                    print(f\"    y_train_raw: {DATASETS[horizon]['y_train_raw'].shape}\")\n",
    "else:\n",
    "    print(\"âŒ No DATASETS found! Checking for parquet files...\")\n",
    "\n",
    "# Load main DataFrames from parquet files\n",
    "print(\"\\nðŸ’¾ Loading DataFrames...\")\n",
    "dataframes_to_load = {}\n",
    "\n",
    "# Check for parquet files in datasets folder\n",
    "if datasets_folders:\n",
    "    latest_dataset = sorted(datasets_folders)[-1]\n",
    "    parquet_files = list(latest_dataset.glob(\"*.parquet\"))\n",
    "    \n",
    "    for pq_file in parquet_files:\n",
    "        try:\n",
    "            df_name = pq_file.stem\n",
    "            dataframes_to_load[df_name] = pd.read_parquet(pq_file)\n",
    "            print(f\"  âœ“ {df_name}: {dataframes_to_load[df_name].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Failed to load {pq_file.name}: {e}\")\n",
    "\n",
    "# Assign to variables\n",
    "if \"features\" in dataframes_to_load:\n",
    "    features_df = dataframes_to_load[\"features\"]\n",
    "if \"targets\" in dataframes_to_load:\n",
    "    targets_df = dataframes_to_load[\"targets\"]\n",
    "if \"returns\" in dataframes_to_load:\n",
    "    returns_df = dataframes_to_load[\"returns\"]\n",
    "if \"targets_all\" in dataframes_to_load:\n",
    "    targets_all = dataframes_to_load[\"targets_all\"]\n",
    "if \"cleaned_prices\" in dataframes_to_load:\n",
    "    cleaned_prices = dataframes_to_load[\"cleaned_prices\"]\n",
    "\n",
    "# ----- PART 4: SET GLOBALS FOR CELLS 7-10 -----\n",
    "print(\"\\nðŸ”§ PART 4: Setting up globals for Cells 7-10...\")\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Asset universe (from your Cell 1)\n",
    "ASSET_UNIVERSE = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOGL', 'TSLA', 'META', 'JPM', 'KO', 'DIS',\n",
    "                  'EURUSD=X', 'GBPUSD=X', 'USDJPY=X', 'EURGBP=X', 'USDCHF=X']\n",
    "STOCKS = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOGL', 'TSLA', 'META', 'JPM', 'KO', 'DIS']\n",
    "FX_PAIRS = ['EURUSD=X', 'GBPUSD=X', 'USDJPY=X', 'EURGBP=X', 'USDCHF=X']\n",
    "\n",
    "# Update globals dictionary\n",
    "globals().update({\n",
    "    \"PROJECT_DIRS\": PROJECT_DIRS,\n",
    "    \"BASE_DIR\": BASE_DIR,\n",
    "    \"DATASETS\": DATASETS,\n",
    "    \"FORECAST_HORIZONS\": FORECAST_HORIZONS,\n",
    "    \"MAX_HORIZON\": MAX_HORIZON,\n",
    "    \"TEMPORAL_SPLITS\": TEMPORAL_SPLITS,\n",
    "    \"ASSET_UNIVERSE\": ASSET_UNIVERSE,\n",
    "    \"STOCKS\": STOCKS,\n",
    "    \"FX_PAIRS\": FX_PAIRS,\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"RANDOM_SEED\": RANDOM_SEED,\n",
    "    \"metadata\": metadata,\n",
    "})\n",
    "\n",
    "# Also add DataFrames to globals if they exist\n",
    "for var_name in [\"features_df\", \"targets_df\", \"returns_df\", \"targets_all\", \"cleaned_prices\"]:\n",
    "    if var_name in locals():\n",
    "        globals()[var_name] = locals()[var_name]\n",
    "        print(f\"  âœ… {var_name} available\")\n",
    "\n",
    "# ----- PART 5: VERIFICATION -----\n",
    "print(\"\\nðŸ” PART 5: Verification...\")\n",
    "print(f\"âœ… DATASETS loaded: {len(DATASETS)} horizons\")\n",
    "print(f\"âœ… FORECAST_HORIZONS: {FORECAST_HORIZONS}\")\n",
    "print(f\"âœ… PROJECT_DIRS set up: {len(PROJECT_DIRS)} directories\")\n",
    "print(f\"âœ… DEVICE: {DEVICE}\")\n",
    "\n",
    "# Test DATASETS structure\n",
    "if DATASETS:\n",
    "    sample_horizon = list(DATASETS.keys())[0]\n",
    "    sample_data = DATASETS[sample_horizon]\n",
    "    print(f\"\\nðŸ“Š Sample horizon '{sample_horizon}':\")\n",
    "    \n",
    "    # Count different types of arrays\n",
    "    x_keys = [k for k in sample_data.keys() if k.startswith('X_')]\n",
    "    y_keys = [k for k in sample_data.keys() if k.startswith('y_')]\n",
    "    \n",
    "    print(f\"  â€¢ X arrays: {len(x_keys)} (train/val/test)\")\n",
    "    print(f\"  â€¢ y arrays: {len(y_keys)} (raw/volnorm/dir/cls/etc.)\")\n",
    "    \n",
    "    # Show some shapes\n",
    "    if \"X_train\" in sample_data:\n",
    "        print(f\"  â€¢ X_train shape: {sample_data['X_train'].shape}\")\n",
    "    if \"y_train_raw\" in sample_data:\n",
    "        print(f\"  â€¢ y_train_raw shape: {sample_data['y_train_raw'].shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ¯ READY FOR CELLS 7-10!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸ“‹ You can now copy-paste your Cells 7, 8, 9, 10 from Colab.\")\n",
    "print(\"ðŸ”§ Required globals are available:\")\n",
    "print(\"   â€¢ DATASETS\")\n",
    "print(\"   â€¢ PROJECT_DIRS\") \n",
    "print(\"   â€¢ FORECAST_HORIZONS\")\n",
    "print(\"   â€¢ ASSET_UNIVERSE, STOCKS, FX_PAIRS\")\n",
    "print(\"   â€¢ DEVICE (GPU ready)\")\n",
    "print(\"   â€¢ RANDOM_SEED\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd8c97c-cf4b-4c2a-96be-4d185fe246af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ MANUAL LOAD\n",
      "Extracting ./quantgenius_complete_20260116_235212.zip to /workspace/\n",
      "\n",
      "Found 7 datasets folders:\n",
      "  â€¢ /workspace/data/datasets_20260116_234738\n",
      "  â€¢ /workspace/data/datasets_20260116_234738/intermediate_10d\n",
      "  â€¢ /workspace/data/datasets_20260116_234738/intermediate_15d\n",
      "  â€¢ /workspace/data/datasets_20260116_234738/intermediate_20d\n",
      "  â€¢ /workspace/data/datasets_20260116_234738/short_1d\n",
      "  â€¢ /workspace/data/datasets_20260116_234738/short_3d\n",
      "  â€¢ /workspace/data/datasets_20260116_234738/short_5d\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# ðŸš€ QUICK FIX: Manual Load\n",
    "# ================================\n",
    "print(\"ðŸš€ MANUAL LOAD\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Re-extract manually\n",
    "zip_path = \"./quantgenius_complete_20260116_235212.zip\"\n",
    "extract_to = \"/workspace/\"\n",
    "\n",
    "print(f\"Extracting {zip_path} to {extract_to}\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "# Check what we got\n",
    "import glob\n",
    "all_folders = glob.glob(\"/workspace/**\", recursive=True)\n",
    "datasets_folders = [f for f in all_folders if \"datasets_\" in f and os.path.isdir(f)]\n",
    "\n",
    "print(f\"\\nFound {len(datasets_folders)} datasets folders:\")\n",
    "for f in datasets_folders:\n",
    "    print(f\"  â€¢ {f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ab0765-c47d-40c1-99fd-704570ed7e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… FINAL: LOADING DATASETS\n",
      "================================================================================\n",
      "ðŸ“‚ Loading from: /workspace/data/datasets_20260116_234738\n",
      "\n",
      "ðŸ§© Loading numpy arrays...\n",
      "  âœ… short_1d: 21 arrays\n",
      "    X_train: (20130, 52)\n",
      "    y_train_raw: (20130,)\n",
      "  âœ… short_3d: 21 arrays\n",
      "    X_train: (20130, 52)\n",
      "    y_train_raw: (20130,)\n",
      "  âœ… short_5d: 21 arrays\n",
      "    X_train: (20130, 52)\n",
      "    y_train_raw: (20130,)\n",
      "  âœ… intermediate_10d: 21 arrays\n",
      "    X_train: (20130, 52)\n",
      "    y_train_raw: (20130,)\n",
      "  âœ… intermediate_15d: 21 arrays\n",
      "    X_train: (20130, 52)\n",
      "    y_train_raw: (20130,)\n",
      "  âœ… intermediate_20d: 21 arrays\n",
      "    X_train: (20130, 52)\n",
      "    y_train_raw: (20130,)\n",
      "\n",
      "âœ… DATASETS loaded: 6 horizons\n",
      "\n",
      "ðŸ’¾ Loading DataFrames...\n",
      "  âœ… targets_all: (2756, 360)\n",
      "  âœ… targets: (2756, 60)\n",
      "  âœ… log_prices: (2777, 10)\n",
      "  âœ… features: (2756, 420)\n",
      "  âœ… returns: (2756, 10)\n",
      "  âœ… cleaned_prices: (2777, 10)\n",
      "âš ï¸ Using empty TEMPORAL_SPLITS\n",
      "\n",
      "ðŸ–¥ï¸ Device: cuda\n",
      "âœ… GPU: NVIDIA GeForce RTX 3080 Ti\n",
      "  âœ… features_df added to globals\n",
      "  âœ… targets_df added to globals\n",
      "  âœ… returns_df added to globals\n",
      "  âœ… targets_all added to globals\n",
      "  âœ… cleaned_prices added to globals\n",
      "  âœ… log_prices added to globals\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ READY FOR CELLS 7-10!\n",
      "================================================================================\n",
      "\n",
      "âœ… DATASETS loaded with arrays\n",
      "âœ… PROJECT_DIRS set up\n",
      "âœ… All required globals available\n",
      "\n",
      "ðŸ“‹ You can now copy-paste your Cells 7, 8, 9, 10\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# âœ… FINAL: LOAD DATASETS PROPERLY\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… FINAL: LOADING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# The correct path\n",
    "DATASETS_PATH = Path(\"/workspace/data/datasets_20260116_234738\")\n",
    "print(f\"ðŸ“‚ Loading from: {DATASETS_PATH}\")\n",
    "\n",
    "# Load DATASETS\n",
    "DATASETS = {}\n",
    "horizons = [\"short_1d\", \"short_3d\", \"short_5d\", \"intermediate_10d\", \"intermediate_15d\", \"intermediate_20d\"]\n",
    "\n",
    "print(\"\\nðŸ§© Loading numpy arrays...\")\n",
    "for horizon in horizons:\n",
    "    horizon_dir = DATASETS_PATH / horizon\n",
    "    if horizon_dir.exists():\n",
    "        DATASETS[horizon] = {}\n",
    "        npy_files = list(horizon_dir.glob(\"*.npy\"))\n",
    "        \n",
    "        for npy_file in npy_files:\n",
    "            try:\n",
    "                DATASETS[horizon][npy_file.stem] = np.load(npy_file)\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Failed to load {npy_file.name}: {e}\")\n",
    "        \n",
    "        print(f\"  âœ… {horizon}: {len(npy_files)} arrays\")\n",
    "        \n",
    "        # Show key shapes\n",
    "        if \"X_train\" in DATASETS[horizon]:\n",
    "            print(f\"    X_train: {DATASETS[horizon]['X_train'].shape}\")\n",
    "        if \"y_train_raw\" in DATASETS[horizon]:\n",
    "            print(f\"    y_train_raw: {DATASETS[horizon]['y_train_raw'].shape}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {horizon}: not found\")\n",
    "\n",
    "print(f\"\\nâœ… DATASETS loaded: {len(DATASETS)} horizons\")\n",
    "\n",
    "# Load DataFrames (parquet files)\n",
    "print(\"\\nðŸ’¾ Loading DataFrames...\")\n",
    "dataframes = {}\n",
    "for parquet_file in DATASETS_PATH.glob(\"*.parquet\"):\n",
    "    try:\n",
    "        df_name = parquet_file.stem\n",
    "        dataframes[df_name] = pd.read_parquet(parquet_file)\n",
    "        print(f\"  âœ… {df_name}: {dataframes[df_name].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Failed to load {parquet_file.name}: {e}\")\n",
    "\n",
    "# Assign to variables\n",
    "if \"features\" in dataframes:\n",
    "    features_df = dataframes[\"features\"]\n",
    "if \"targets\" in dataframes:\n",
    "    targets_df = dataframes[\"targets\"]\n",
    "if \"returns\" in dataframes:\n",
    "    returns_df = dataframes[\"returns\"]\n",
    "if \"targets_all\" in dataframes:\n",
    "    targets_all = dataframes[\"targets_all\"]\n",
    "if \"cleaned_prices\" in dataframes:\n",
    "    cleaned_prices = dataframes[\"cleaned_prices\"]\n",
    "if \"log_prices\" in dataframes:\n",
    "    log_prices = dataframes[\"log_prices\"]\n",
    "\n",
    "# Set up PROJECT_DIRS\n",
    "PROJECT_DIRS = {\n",
    "    \"data\": Path(\"/workspace/quantgenius_project/data\"),\n",
    "    \"figures\": Path(\"/workspace/quantgenius_project/figures\"),\n",
    "    \"tables\": Path(\"/workspace/quantgenius_project/tables\"),\n",
    "    \"models\": Path(\"/workspace/quantgenius_project/models\"),\n",
    "    \"reports\": Path(\"/workspace/quantgenius_project/reports\"),\n",
    "    \"cache\": Path(\"/workspace/quantgenius_project/cache\")\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for name, path in PROJECT_DIRS.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set other required globals\n",
    "FORECAST_HORIZONS = {\n",
    "    \"short_1d\": 1, \"short_3d\": 3, \"short_5d\": 5,\n",
    "    \"intermediate_10d\": 10, \"intermediate_15d\": 15, \"intermediate_20d\": 20\n",
    "}\n",
    "MAX_HORIZON = max(FORECAST_HORIZONS.values())\n",
    "\n",
    "ASSET_UNIVERSE = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOGL', 'TSLA', 'META', 'JPM', 'KO', 'DIS',\n",
    "                  'EURUSD=X', 'GBPUSD=X', 'USDJPY=X', 'EURGBP=X', 'USDCHF=X']\n",
    "STOCKS = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOGL', 'TSLA', 'META', 'JPM', 'KO', 'DIS']\n",
    "FX_PAIRS = ['EURUSD=X', 'GBPUSD=X', 'USDJPY=X', 'EURGBP=X', 'USDCHF=X']\n",
    "\n",
    "# TEMPORAL_SPLITS (you might need to load from metadata)\n",
    "# Try to find metadata\n",
    "import json, glob\n",
    "metadata_files = glob.glob(\"/workspace/quantgenius_project/tables/targets_metadata_*.json\")\n",
    "if metadata_files:\n",
    "    with open(sorted(metadata_files)[-1], 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    TEMPORAL_SPLITS = metadata.get(\"temporal_splits\", {})\n",
    "    print(f\"âœ… Loaded TEMPORAL_SPLITS from metadata\")\n",
    "else:\n",
    "    TEMPORAL_SPLITS = {}\n",
    "    print(\"âš ï¸ Using empty TEMPORAL_SPLITS\")\n",
    "\n",
    "# Device\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"\\nðŸ–¥ï¸ Device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Update globals\n",
    "globals().update({\n",
    "    \"DATASETS\": DATASETS,\n",
    "    \"PROJECT_DIRS\": PROJECT_DIRS,\n",
    "    \"FORECAST_HORIZONS\": FORECAST_HORIZONS,\n",
    "    \"MAX_HORIZON\": MAX_HORIZON,\n",
    "    \"ASSET_UNIVERSE\": ASSET_UNIVERSE,\n",
    "    \"STOCKS\": STOCKS,\n",
    "    \"FX_PAIRS\": FX_PAIRS,\n",
    "    \"TEMPORAL_SPLITS\": TEMPORAL_SPLITS,\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"RANDOM_SEED\": RANDOM_SEED,\n",
    "})\n",
    "\n",
    "# Add DataFrames to globals\n",
    "for var_name in [\"features_df\", \"targets_df\", \"returns_df\", \"targets_all\", \"cleaned_prices\", \"log_prices\"]:\n",
    "    if var_name in locals():\n",
    "        globals()[var_name] = locals()[var_name]\n",
    "        print(f\"  âœ… {var_name} added to globals\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ¯ READY FOR CELLS 7-10!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nâœ… DATASETS loaded with arrays\")\n",
    "print(\"âœ… PROJECT_DIRS set up\")\n",
    "print(\"âœ… All required globals available\")\n",
    "print(\"\\nðŸ“‹ You can now copy-paste your Cells 7, 8, 9, 10\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0c5e4a-fad4-4668-b22b-4237e6ce5756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================================================================\n",
      "ðŸŽ¯ CELL 7 (THESIS-GRADE): XGBoost Training + Finance-Grade Evaluation (GPU/CPU)\n",
      "==============================================================================================================\n",
      "âœ… XGBoost device: cuda | GPU available: True\n",
      "\n",
      "ðŸ” DATASETS sanity check:\n",
      "Sample horizon: short_1d\n",
      "Keys: ['X_test', 'X_train', 'X_val', 'y_test_cls', 'y_test_crossrank', 'y_test_dir', 'y_test_extreme', 'y_test_raw', 'y_test_volnorm', 'y_train_cls', 'y_train_crossrank', 'y_train_dir', 'y_train_extreme', 'y_train_raw', 'y_train_volnorm', 'y_val_cls', 'y_val_crossrank', 'y_val_dir', 'y_val_extreme', 'y_val_raw', 'y_val_volnorm'] ...\n",
      "\n",
      "==============================================================================================================\n",
      "ðŸ“‰ REGRESSION TRAINING (XGBoost) â€” targets: ['y_volnorm', 'y_raw', 'y_crossrank']\n",
      "==============================================================================================================\n",
      "\n",
      "ðŸŽ¯ short_1d (1d) | Regression target: y_volnorm\n",
      "   Train: (19940, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0002 | IC=0.0033 | DirAcc=0.535 | Sharpe=0.811\n",
      "\n",
      "ðŸŽ¯ short_1d (1d) | Regression target: y_raw\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=0.0003 | IC=0.0063 | DirAcc=0.535 | Sharpe=0.749\n",
      "\n",
      "ðŸŽ¯ short_1d (1d) | Regression target: y_crossrank\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0023 | IC=0.0133 | DirAcc=0.501 | Sharpe=0.823\n",
      "\n",
      "ðŸŽ¯ short_3d (3d) | Regression target: y_volnorm\n",
      "   Train: (19940, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=0.0002 | IC=-0.0037 | DirAcc=0.554 | Sharpe=0.831\n",
      "\n",
      "ðŸŽ¯ short_3d (3d) | Regression target: y_raw\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0008 | IC=-0.0093 | DirAcc=0.554 | Sharpe=0.831\n",
      "\n",
      "ðŸŽ¯ short_3d (3d) | Regression target: y_crossrank\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0032 | IC=0.0189 | DirAcc=0.501 | Sharpe=0.843\n",
      "\n",
      "ðŸŽ¯ short_5d (5d) | Regression target: y_volnorm\n",
      "   Train: (19940, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=0.0005 | IC=0.0488 | DirAcc=0.564 | Sharpe=0.849\n",
      "\n",
      "ðŸŽ¯ short_5d (5d) | Regression target: y_raw\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0009 | IC=0.0291 | DirAcc=0.569 | Sharpe=1.027\n",
      "\n",
      "ðŸŽ¯ short_5d (5d) | Regression target: y_crossrank\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0090 | IC=0.0195 | DirAcc=0.497 | Sharpe=0.814\n",
      "\n",
      "ðŸŽ¯ intermediate_10d (10d) | Regression target: y_volnorm\n",
      "   Train: (19940, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=0.0015 | IC=0.0587 | DirAcc=0.579 | Sharpe=0.834\n",
      "\n",
      "ðŸŽ¯ intermediate_10d (10d) | Regression target: y_raw\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0023 | IC=0.0674 | DirAcc=0.579 | Sharpe=0.834\n",
      "\n",
      "ðŸŽ¯ intermediate_10d (10d) | Regression target: y_crossrank\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=0.0011 | IC=0.0405 | DirAcc=0.500 | Sharpe=0.834\n",
      "\n",
      "ðŸŽ¯ intermediate_15d (15d) | Regression target: y_volnorm\n",
      "   Train: (19940, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=0.0045 | IC=0.0485 | DirAcc=0.592 | Sharpe=0.834\n",
      "\n",
      "ðŸŽ¯ intermediate_15d (15d) | Regression target: y_raw\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0088 | IC=0.0336 | DirAcc=0.587 | Sharpe=0.788\n",
      "\n",
      "ðŸŽ¯ intermediate_15d (15d) | Regression target: y_crossrank\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0011 | IC=-0.0534 | DirAcc=0.500 | Sharpe=0.834\n",
      "\n",
      "ðŸŽ¯ intermediate_20d (20d) | Regression target: y_volnorm\n",
      "   Train: (19940, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0094 | IC=0.0322 | DirAcc=0.594 | Sharpe=0.718\n",
      "\n",
      "ðŸŽ¯ intermediate_20d (20d) | Regression target: y_raw\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0862 | IC=-0.0426 | DirAcc=0.554 | Sharpe=0.258\n",
      "\n",
      "ðŸŽ¯ intermediate_20d (20d) | Regression target: y_crossrank\n",
      "   Train: (20130, 52) | Val: (2500, 52) | Test: (4930, 52)\n",
      "   âœ… R2=-0.0037 | IC=0.0221 | DirAcc=0.496 | Sharpe=0.598\n",
      "\n",
      "==============================================================================================================\n",
      "ðŸ“Œ CLASSIFICATION TRAINING (XGBoost) â€” target: y_cls (-1/0/1)\n",
      "==============================================================================================================\n",
      "\n",
      "ðŸŽ¯ short_1d (1d) | Classification target: y_cls\n",
      "   Train: (22630, 52) | Test: (4930, 52) | Class counts: [7227, 6940, 8463]\n",
      "   âœ… acc=0.379 | bal_acc=0.375 | macro_f1=0.375\n",
      "\n",
      "ðŸŽ¯ short_3d (3d) | Classification target: y_cls\n",
      "   Train: (22630, 52) | Test: (4930, 52) | Class counts: [8247, 3810, 10573]\n",
      "   âœ… acc=0.409 | bal_acc=0.343 | macro_f1=0.342\n",
      "\n",
      "ðŸŽ¯ short_5d (5d) | Classification target: y_cls\n",
      "   Train: (22630, 52) | Test: (4930, 52) | Class counts: [8366, 2881, 11383]\n",
      "   âœ… acc=0.447 | bal_acc=0.348 | macro_f1=0.346\n",
      "\n",
      "ðŸŽ¯ intermediate_10d (10d) | Classification target: y_cls\n",
      "   Train: (22630, 52) | Test: (4930, 52) | Class counts: [8368, 1989, 12273]\n",
      "   âœ… acc=0.451 | bal_acc=0.329 | macro_f1=0.327\n",
      "\n",
      "ðŸŽ¯ intermediate_15d (15d) | Classification target: y_cls\n",
      "   Train: (22630, 52) | Test: (4930, 52) | Class counts: [8198, 1532, 12900]\n",
      "   âœ… acc=0.476 | bal_acc=0.346 | macro_f1=0.346\n",
      "\n",
      "ðŸŽ¯ intermediate_20d (20d) | Classification target: y_cls\n",
      "   Train: (22630, 52) | Test: (4930, 52) | Class counts: [8048, 1263, 13319]\n",
      "   âœ… acc=0.478 | bal_acc=0.329 | macro_f1=0.324\n",
      "\n",
      "==============================================================================================================\n",
      "âœ… CELL 7 COMPLETE (THESIS-GRADE)\n",
      "ðŸ“ Results CSV : /workspace/quantgenius_project/tables/cell7_xgb_thesis_results_cuda_20260117_142617.csv\n",
      "ðŸ“ Meta JSON   : /workspace/quantgenius_project/tables/cell7_xgb_thesis_meta_cuda_20260117_142617.json\n",
      "ðŸ“ Models dir  : /workspace/quantgenius_project/models\n",
      "==============================================================================================================\n",
      "\n",
      "ðŸ“‹ QUICK SUMMARY (sorted):\n",
      "          task      target          horizon  horizon_days        R2        IC   DirAcc   Sharpe      acc  balanced_acc  macro_f1  n_train  n_test\n",
      "classification       y_cls         short_1d             1       NaN       NaN      NaN      NaN 0.379108      0.375108  0.375441    22630    4930\n",
      "classification       y_cls         short_3d             3       NaN       NaN      NaN      NaN 0.408722      0.343316  0.341923    22630    4930\n",
      "classification       y_cls         short_5d             5       NaN       NaN      NaN      NaN 0.446653      0.348220  0.346180    22630    4930\n",
      "classification       y_cls intermediate_10d            10       NaN       NaN      NaN      NaN 0.450710      0.329103  0.327170    22630    4930\n",
      "classification       y_cls intermediate_15d            15       NaN       NaN      NaN      NaN 0.476065      0.345754  0.345727    22630    4930\n",
      "classification       y_cls intermediate_20d            20       NaN       NaN      NaN      NaN 0.477688      0.328943  0.324370    22630    4930\n",
      "    regression y_crossrank         short_1d             1 -0.002252  0.013347 0.501014 0.823482      NaN           NaN       NaN    20130    4930\n",
      "    regression y_crossrank         short_3d             3 -0.003240  0.018945 0.501420 0.843127      NaN           NaN       NaN    20130    4930\n",
      "    regression y_crossrank         short_5d             5 -0.009049  0.019472 0.496552 0.813543      NaN           NaN       NaN    20130    4930\n",
      "    regression y_crossrank intermediate_10d            10  0.001123  0.040476 0.500000 0.834458      NaN           NaN       NaN    20130    4930\n",
      "    regression y_crossrank intermediate_15d            15 -0.001137 -0.053425 0.500000 0.834270      NaN           NaN       NaN    20130    4930\n",
      "    regression y_crossrank intermediate_20d            20 -0.003705  0.022137 0.495740 0.598129      NaN           NaN       NaN    20130    4930\n",
      "    regression       y_raw         short_1d             1  0.000346  0.006315 0.535294 0.749260      NaN           NaN       NaN    20130    4930\n",
      "    regression       y_raw         short_3d             3 -0.000817 -0.009348 0.554158 0.830795      NaN           NaN       NaN    20130    4930\n",
      "    regression       y_raw         short_5d             5 -0.000927  0.029125 0.568763 1.027125      NaN           NaN       NaN    20130    4930\n",
      "    regression       y_raw intermediate_10d            10 -0.002302  0.067449 0.578702 0.834458      NaN           NaN       NaN    20130    4930\n",
      "    regression       y_raw intermediate_15d            15 -0.008829  0.033629 0.587424 0.787667      NaN           NaN       NaN    20130    4930\n",
      "    regression       y_raw intermediate_20d            20 -0.086152 -0.042574 0.553753 0.257905      NaN           NaN       NaN    20130    4930\n",
      "    regression   y_volnorm         short_1d             1 -0.000243  0.003325 0.534888 0.810756      NaN           NaN       NaN    19940    4930\n",
      "    regression   y_volnorm         short_3d             3  0.000199 -0.003662 0.554158 0.830795      NaN           NaN       NaN    19940    4930\n",
      "    regression   y_volnorm         short_5d             5  0.000504  0.048763 0.563895 0.849329      NaN           NaN       NaN    19940    4930\n",
      "    regression   y_volnorm intermediate_10d            10  0.001505  0.058734 0.578702 0.834458      NaN           NaN       NaN    19940    4930\n",
      "    regression   y_volnorm intermediate_15d            15  0.004523  0.048525 0.592089 0.834270      NaN           NaN       NaN    19940    4930\n",
      "    regression   y_volnorm intermediate_20d            20 -0.009444  0.032190 0.593712 0.717549      NaN           NaN       NaN    19940    4930\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CELL 7 (THESIS-GRADE): XGBoost Training + Finance-Grade Evaluation (GPU/CPU) \n",
    "# ================================\n",
    "# Works with your existing structure from Cell 5:\n",
    "# DATASETS[horizon] contains:\n",
    "#   X_train, X_val, X_test (float32)\n",
    "#   y_* arrays (float32/int32): y_train_raw, y_train_volnorm, y_train_crossrank, y_train_cls, y_train_dir, ...\n",
    "#\n",
    "# This cell:\n",
    "# âœ… Trains XGB regression for: y_volnorm, y_raw, y_crossrank (optional switch)\n",
    "# âœ… Trains XGB classification for: y_cls (-1/0/1) with class-weighted sampling\n",
    "# âœ… Reports finance-grade metrics:\n",
    "#    - Regression: R2, RMSE, MAE, DirAcc, IC (Spearman), Sharpe proxy (non-overlap), Turnover\n",
    "#    - Classification: Acc, BalancedAcc, MacroF1, ConfusionMatrix\n",
    "# âœ… Saves: results CSV + meta JSON + models\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"ðŸŽ¯ CELL 7 (THESIS-GRADE): XGBoost Training + Finance-Grade Evaluation (GPU/CPU)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ----------------\n",
    "# 0) Checks\n",
    "# ----------------\n",
    "required = [\"DATASETS\", \"FORECAST_HORIZONS\", \"PROJECT_DIRS\"]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing required globals: {missing}. Run Cells 1â€“6 first.\")\n",
    "\n",
    "tbl_dir = PROJECT_DIRS[\"tables\"]\n",
    "mdl_dir = PROJECT_DIRS.get(\"models\", PROJECT_DIRS[\"tables\"].parent / \"models\")\n",
    "tbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "mdl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "# ----------------\n",
    "# 1) Device detection (robust)\n",
    "# ----------------\n",
    "def detect_xgb_device():\n",
    "    \"\"\"\n",
    "    Returns (gpu_available, device_string) where device_string is 'cuda' or 'cpu'.\n",
    "    Compatible with XGBoost >= 2.x device API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # tiny fit\n",
    "        m = xgb.XGBRegressor(\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            n_estimators=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        X_tmp = np.random.randn(64, 8).astype(np.float32)\n",
    "        y_tmp = np.random.randn(64).astype(np.float32)\n",
    "        m.fit(X_tmp, y_tmp, verbose=False)\n",
    "        return True, \"cuda\"\n",
    "    except Exception:\n",
    "        return False, \"cpu\"\n",
    "\n",
    "GPU_AVAILABLE, XGB_DEVICE = detect_xgb_device()\n",
    "print(f\"âœ… XGBoost device: {XGB_DEVICE} | GPU available: {GPU_AVAILABLE}\")\n",
    "\n",
    "# ----------------\n",
    "# 2) Finance-grade metric helpers\n",
    "# ----------------\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() < 10:\n",
    "        return np.nan\n",
    "    return float((np.sign(y_true[mask]) == np.sign(y_pred[mask])).mean())\n",
    "\n",
    "def spearman_ic(y_true, y_pred):\n",
    "    \"\"\"Spearman IC (information coefficient).\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() < 50:\n",
    "        return 0.0\n",
    "    if np.std(y_pred[mask]) < EPS:\n",
    "        return 0.0\n",
    "    ic = stats.spearmanr(y_true[mask], y_pred[mask]).correlation\n",
    "    return float(ic) if ic == ic else 0.0\n",
    "\n",
    "def sharpe_non_overlap(y_true_ret_raw, y_pred, h_days, cost_bps=5.0):\n",
    "    \"\"\"\n",
    "    Strategy proxy:\n",
    "    - position = sign(pred)\n",
    "    - pnl = position * raw_return\n",
    "    - evaluate on non-overlapping blocks (every h_days)\n",
    "    - apply simple turnover cost (bps)\n",
    "    Returns (sharpe, turnover)\n",
    "    \"\"\"\n",
    "    y_true_ret_raw = np.asarray(y_true_ret_raw, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(y_true_ret_raw) & np.isfinite(y_pred)\n",
    "    if mask.sum() < 50:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    y = y_true_ret_raw[mask]\n",
    "    p = y_pred[mask]\n",
    "\n",
    "    step = max(int(h_days), 1)\n",
    "    idx = np.arange(0, len(y), step)\n",
    "    y_s = y[idx]\n",
    "    pos = np.sign(p[idx])  # -1/0/1\n",
    "\n",
    "    pnl = pos * y_s\n",
    "\n",
    "    if len(pos) > 1:\n",
    "        turnover = np.abs(pos[1:] - pos[:-1])\n",
    "        pnl[1:] -= turnover * (cost_bps / 10000.0)\n",
    "        avg_turnover = float(np.mean(turnover))\n",
    "    else:\n",
    "        avg_turnover = 0.0\n",
    "\n",
    "    mu = float(np.mean(pnl))\n",
    "    sd = float(np.std(pnl))\n",
    "    ann = np.sqrt(252 / step)\n",
    "    sharpe = np.nan if sd < EPS else (mu / sd) * ann\n",
    "    return float(sharpe), float(avg_turnover)\n",
    "\n",
    "def reg_metrics(y_true, y_pred, h_days, y_true_raw_for_sharpe=None):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() < 50:\n",
    "        return {\n",
    "            \"R2\": np.nan, \"RMSE\": np.nan, \"MAE\": np.nan,\n",
    "            \"DirAcc\": np.nan, \"IC\": np.nan,\n",
    "            \"Sharpe\": np.nan, \"Turnover\": np.nan,\n",
    "            \"pred_std\": np.nan, \"n_eval\": int(mask.sum())\n",
    "        }\n",
    "\n",
    "    yt = y_true[mask]\n",
    "    yp = y_pred[mask]\n",
    "\n",
    "    rmse = float(np.sqrt(mean_squared_error(yt, yp)))\n",
    "    mae  = float(mean_absolute_error(yt, yp))\n",
    "    r2   = float(r2_score(yt, yp))\n",
    "    da   = directional_accuracy(yt, yp)\n",
    "    ic   = spearman_ic(yt, yp)\n",
    "\n",
    "    # Sharpe proxy should use RAW return if available\n",
    "    if y_true_raw_for_sharpe is None:\n",
    "        y_true_raw_for_sharpe = yt\n",
    "\n",
    "    sharpe, to = sharpe_non_overlap(y_true_raw_for_sharpe, yp, h_days)\n",
    "\n",
    "    return {\n",
    "        \"R2\": r2, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"DirAcc\": da, \"IC\": ic,\n",
    "        \"Sharpe\": sharpe, \"Turnover\": to,\n",
    "        \"pred_std\": float(np.std(yp)),\n",
    "        \"n_eval\": int(len(yt))\n",
    "    }\n",
    "\n",
    "def cls_metrics(y_true_cls, y_pred_cls):\n",
    "    \"\"\"\n",
    "    y_true_cls and y_pred_cls are in {-1,0,1}.\n",
    "    \"\"\"\n",
    "    y_true_cls = np.asarray(y_true_cls)\n",
    "    y_pred_cls = np.asarray(y_pred_cls)\n",
    "    mask = np.isfinite(y_true_cls) & np.isfinite(y_pred_cls)\n",
    "    if mask.sum() < 50:\n",
    "        return {\"acc\": np.nan, \"balanced_acc\": np.nan, \"macro_f1\": np.nan, \"cm\": []}\n",
    "\n",
    "    yt = y_true_cls[mask].astype(int)\n",
    "    yp = y_pred_cls[mask].astype(int)\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(yt, yp)),\n",
    "        \"balanced_acc\": float(balanced_accuracy_score(yt, yp)),\n",
    "        \"macro_f1\": float(f1_score(yt, yp, average=\"macro\", zero_division=0)),\n",
    "        \"cm\": confusion_matrix(yt, yp, labels=[-1, 0, 1]).tolist()\n",
    "    }\n",
    "\n",
    "# ----------------\n",
    "# 3) Training settings (thesis-grade defaults)\n",
    "# ----------------\n",
    "# You can switch these without breaking anything.\n",
    "TRAIN_REG_TARGETS = [\"y_volnorm\", \"y_raw\", \"y_crossrank\"]  # keep all for a strong thesis table\n",
    "TRAIN_CLASSIFICATION = True\n",
    "\n",
    "# Regression hyperparams: robust to fat tails\n",
    "REG_PARAMS = dict(\n",
    "    objective=\"reg:pseudohubererror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    tree_method=\"hist\",\n",
    "    device=XGB_DEVICE,\n",
    "    random_state=42,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=2.0,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Classification hyperparams (ternary)\n",
    "CLS_PARAMS = dict(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    device=XGB_DEVICE,\n",
    "    random_state=42,\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    min_child_weight=10,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=2.0,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 150  # used when val exists\n",
    "COST_BPS_FOR_SHARPE = 5.0\n",
    "\n",
    "# ----------------\n",
    "# 4) Quick dataset sanity\n",
    "# ----------------\n",
    "print(\"\\nðŸ” DATASETS sanity check:\")\n",
    "sample_hz = list(DATASETS.keys())[0]\n",
    "print(f\"Sample horizon: {sample_hz}\")\n",
    "print(\"Keys:\", sorted([k for k in DATASETS[sample_hz].keys() if k.startswith((\"X_\", \"y_\"))])[:25], \"...\")\n",
    "\n",
    "# ----------------\n",
    "# 5) Training loops\n",
    "# ----------------\n",
    "results = []\n",
    "meta_runs = []\n",
    "\n",
    "def get_reg_keys(tgt):\n",
    "    # maps to your dataset keys from Cell 5\n",
    "    return f\"y_train_{tgt.split('_', 1)[1]}\", f\"y_val_{tgt.split('_', 1)[1]}\", f\"y_test_{tgt.split('_', 1)[1]}\"\n",
    "\n",
    "def resolve_reg_arrays(ds, tgt_name):\n",
    "    \"\"\"\n",
    "    tgt_name in {\"y_volnorm\",\"y_raw\",\"y_crossrank\"}\n",
    "    returns (ytr, yva, yte) or (None,..) if missing.\n",
    "    \"\"\"\n",
    "    if tgt_name == \"y_volnorm\":\n",
    "        return ds.get(\"y_train_volnorm\"), ds.get(\"y_val_volnorm\"), ds.get(\"y_test_volnorm\")\n",
    "    if tgt_name == \"y_raw\":\n",
    "        return ds.get(\"y_train_raw\"), ds.get(\"y_val_raw\"), ds.get(\"y_test_raw\")\n",
    "    if tgt_name == \"y_crossrank\":\n",
    "        return ds.get(\"y_train_crossrank\"), ds.get(\"y_val_crossrank\"), ds.get(\"y_test_crossrank\")\n",
    "    return None, None, None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"ðŸ“‰ REGRESSION TRAINING (XGBoost) â€” targets:\", TRAIN_REG_TARGETS)\n",
    "print(\"=\" * 110)\n",
    "\n",
    "for hz, h_days in FORECAST_HORIZONS.items():\n",
    "    if hz not in DATASETS:\n",
    "        print(f\"âš ï¸ {hz}: not found in DATASETS, skipping.\")\n",
    "        continue\n",
    "\n",
    "    ds = DATASETS[hz]\n",
    "    Xtr = ds.get(\"X_train\")\n",
    "    Xva = ds.get(\"X_val\")\n",
    "    Xte = ds.get(\"X_test\")\n",
    "\n",
    "    # raw returns used for Sharpe proxy\n",
    "    yte_raw_for_sharpe = ds.get(\"y_test_raw\", None)\n",
    "\n",
    "    if Xtr is None or Xte is None:\n",
    "        print(f\"âŒ {hz}: Missing X_train or X_test. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for tgt in TRAIN_REG_TARGETS:\n",
    "        ytr, yva, yte = resolve_reg_arrays(ds, tgt)\n",
    "        if ytr is None or yte is None:\n",
    "            print(f\"  âš ï¸ {hz}/{tgt}: missing target arrays, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Drop NaNs for training/validation\n",
    "        tr_mask = np.isfinite(ytr)\n",
    "        Xtr_fit = Xtr[tr_mask]\n",
    "        ytr_fit = ytr[tr_mask]\n",
    "\n",
    "        if len(ytr_fit) < 1000:\n",
    "            print(f\"  âš ï¸ {hz}/{tgt}: too few finite train rows ({len(ytr_fit)}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        eval_set = None\n",
    "        if Xva is not None and yva is not None:\n",
    "            va_mask = np.isfinite(yva)\n",
    "            Xva_fit = Xva[va_mask]\n",
    "            yva_fit = yva[va_mask]\n",
    "            if len(yva_fit) >= 200:\n",
    "                eval_set = [(Xva_fit, yva_fit)]\n",
    "            else:\n",
    "                eval_set = None\n",
    "\n",
    "        print(f\"\\nðŸŽ¯ {hz} ({h_days}d) | Regression target: {tgt}\")\n",
    "        print(f\"   Train: {Xtr_fit.shape} | Val: {eval_set[0][0].shape if eval_set else None} | Test: {Xte.shape}\")\n",
    "\n",
    "        model = xgb.XGBRegressor(**REG_PARAMS)\n",
    "        if eval_set:\n",
    "            model.set_params(early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n",
    "            model.fit(Xtr_fit, ytr_fit, eval_set=eval_set, verbose=False)\n",
    "        else:\n",
    "            model.fit(Xtr_fit, ytr_fit, verbose=False)\n",
    "\n",
    "        best_iter = int(getattr(model, \"best_iteration\", model.n_estimators - 1))\n",
    "\n",
    "        # Predict on test\n",
    "        try:\n",
    "            pred = model.predict(Xte, iteration_range=(0, best_iter + 1))\n",
    "        except Exception:\n",
    "            pred = model.predict(Xte)\n",
    "\n",
    "        mets = reg_metrics(\n",
    "            y_true=yte,\n",
    "            y_pred=pred,\n",
    "            h_days=h_days,\n",
    "            y_true_raw_for_sharpe=yte_raw_for_sharpe if yte_raw_for_sharpe is not None else yte\n",
    "        )\n",
    "\n",
    "        print(f\"   âœ… R2={mets['R2']:.4f} | IC={mets['IC']:.4f} | DirAcc={mets['DirAcc']:.3f} | Sharpe={mets['Sharpe']:.3f}\")\n",
    "\n",
    "        # Save model\n",
    "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = mdl_dir / f\"xgb_reg_{tgt}_{hz}_{XGB_DEVICE}_{stamp}.json\"\n",
    "        model.save_model(str(model_path))\n",
    "\n",
    "        results.append({\n",
    "            \"task\": \"regression\",\n",
    "            \"horizon\": hz,\n",
    "            \"horizon_days\": int(h_days),\n",
    "            \"target\": tgt,\n",
    "            \"device\": XGB_DEVICE,\n",
    "            \"best_iter\": best_iter,\n",
    "            \"n_train\": int(len(ytr_fit)),\n",
    "            \"n_test\": int(len(Xte)),\n",
    "            \"model_path\": str(model_path),\n",
    "            **mets\n",
    "        })\n",
    "\n",
    "        meta_runs.append({\n",
    "            \"task\": \"regression\",\n",
    "            \"horizon\": hz,\n",
    "            \"target\": tgt,\n",
    "            \"params\": REG_PARAMS,\n",
    "            \"early_stopping_rounds\": EARLY_STOPPING_ROUNDS if eval_set else None,\n",
    "            \"cost_bps_for_sharpe\": COST_BPS_FOR_SHARPE,\n",
    "            \"best_iteration\": best_iter,\n",
    "            \"model_path\": str(model_path),\n",
    "        })\n",
    "\n",
    "# ----------------\n",
    "# 6) Classification training (ternary y_cls)\n",
    "# ----------------\n",
    "if TRAIN_CLASSIFICATION:\n",
    "    print(\"\\n\" + \"=\" * 110)\n",
    "    print(\"ðŸ“Œ CLASSIFICATION TRAINING (XGBoost) â€” target: y_cls (-1/0/1)\")\n",
    "    print(\"=\" * 110)\n",
    "\n",
    "    for hz, h_days in FORECAST_HORIZONS.items():\n",
    "        if hz not in DATASETS:\n",
    "            continue\n",
    "\n",
    "        ds = DATASETS[hz]\n",
    "        Xtr = ds.get(\"X_train\")\n",
    "        Xva = ds.get(\"X_val\")\n",
    "        Xte = ds.get(\"X_test\")\n",
    "\n",
    "        ytr = ds.get(\"y_train_cls\")\n",
    "        yva = ds.get(\"y_val_cls\")\n",
    "        yte = ds.get(\"y_test_cls\")\n",
    "\n",
    "        if Xtr is None or Xte is None or ytr is None or yte is None:\n",
    "            print(f\"  âš ï¸ {hz}: missing classification arrays, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Combine train+val for fitting (common in thesis baselines)\n",
    "        if Xva is not None and yva is not None:\n",
    "            X_fit = np.vstack([Xtr, Xva])\n",
    "            y_fit = np.concatenate([ytr, yva])\n",
    "        else:\n",
    "            X_fit = Xtr\n",
    "            y_fit = ytr\n",
    "\n",
    "        # Map {-1,0,1} -> {0,1,2}\n",
    "        y_fit = np.asarray(y_fit).astype(int)\n",
    "        y_fit_m = (y_fit + 1).astype(int)\n",
    "        yte = np.asarray(yte).astype(int)\n",
    "        yte_m = (yte + 1).astype(int)\n",
    "\n",
    "        # Class weights (inverse frequency)\n",
    "        counts = np.bincount(y_fit_m, minlength=3).astype(float)\n",
    "        inv = 1.0 / np.maximum(counts, 1.0)\n",
    "        class_w = inv / inv.mean()\n",
    "        sample_w = class_w[y_fit_m]\n",
    "\n",
    "        print(f\"\\nðŸŽ¯ {hz} ({h_days}d) | Classification target: y_cls\")\n",
    "        print(f\"   Train: {X_fit.shape} | Test: {Xte.shape} | Class counts: {counts.astype(int).tolist()}\")\n",
    "\n",
    "        model = xgb.XGBClassifier(**CLS_PARAMS)\n",
    "        model.fit(X_fit, y_fit_m, sample_weight=sample_w, verbose=False)\n",
    "\n",
    "        # Predict -> map back to {-1,0,1}\n",
    "        prob = model.predict_proba(Xte)\n",
    "        pred_m = np.argmax(prob, axis=1).astype(int)\n",
    "        pred = (pred_m - 1).astype(int)\n",
    "\n",
    "        mets = cls_metrics(y_true_cls=yte, y_pred_cls=pred)\n",
    "\n",
    "        print(f\"   âœ… acc={mets['acc']:.3f} | bal_acc={mets['balanced_acc']:.3f} | macro_f1={mets['macro_f1']:.3f}\")\n",
    "\n",
    "        # Save model\n",
    "        stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = mdl_dir / f\"xgb_cls_y_cls_{hz}_{XGB_DEVICE}_{stamp}.json\"\n",
    "        model.save_model(str(model_path))\n",
    "\n",
    "        results.append({\n",
    "            \"task\": \"classification\",\n",
    "            \"horizon\": hz,\n",
    "            \"horizon_days\": int(h_days),\n",
    "            \"target\": \"y_cls\",\n",
    "            \"device\": XGB_DEVICE,\n",
    "            \"best_iter\": np.nan,\n",
    "            \"n_train\": int(len(X_fit)),\n",
    "            \"n_test\": int(len(Xte)),\n",
    "            \"model_path\": str(model_path),\n",
    "            **mets\n",
    "        })\n",
    "\n",
    "        meta_runs.append({\n",
    "            \"task\": \"classification\",\n",
    "            \"horizon\": hz,\n",
    "            \"target\": \"y_cls\",\n",
    "            \"params\": CLS_PARAMS,\n",
    "            \"class_counts\": counts.astype(int).tolist(),\n",
    "            \"class_weights\": class_w.tolist(),\n",
    "            \"model_path\": str(model_path),\n",
    "        })\n",
    "\n",
    "# ----------------\n",
    "# 7) Save results + meta\n",
    "# ----------------\n",
    "if not results:\n",
    "    raise RuntimeError(\"âŒ No models trained. Check DATASETS keys and contents.\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_csv  = tbl_dir / f\"cell7_xgb_thesis_results_{XGB_DEVICE}_{stamp}.csv\"\n",
    "out_json = tbl_dir / f\"cell7_xgb_thesis_meta_{XGB_DEVICE}_{stamp}.json\"\n",
    "\n",
    "results_df.to_csv(out_csv, index=False)\n",
    "\n",
    "meta = {\n",
    "    \"created_at\": stamp,\n",
    "    \"device\": XGB_DEVICE,\n",
    "    \"gpu_available\": bool(GPU_AVAILABLE),\n",
    "    \"horizons\": {k: int(v) for k, v in FORECAST_HORIZONS.items()},\n",
    "    \"trained_regression_targets\": TRAIN_REG_TARGETS,\n",
    "    \"trained_classification\": bool(TRAIN_CLASSIFICATION),\n",
    "    \"reg_params\": REG_PARAMS,\n",
    "    \"cls_params\": CLS_PARAMS,\n",
    "    \"early_stopping_rounds\": EARLY_STOPPING_ROUNDS,\n",
    "    \"cost_bps_for_sharpe\": COST_BPS_FOR_SHARPE,\n",
    "    \"runs\": meta_runs,\n",
    "    \"files\": {\n",
    "        \"results_csv\": str(out_csv)\n",
    "    }\n",
    "}\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# Export globals for Cells 8â€“10\n",
    "globals().update({\n",
    "    \"CELL7_RESULTS_DF\": results_df,\n",
    "    \"CELL7_RESULTS_PATH\": str(out_csv),\n",
    "    \"CELL7_META_PATH\": str(out_json),\n",
    "    \"XGB_MODELS_DIR\": str(mdl_dir)\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"âœ… CELL 7 COMPLETE (THESIS-GRADE)\")\n",
    "print(f\"ðŸ“ Results CSV : {out_csv}\")\n",
    "print(f\"ðŸ“ Meta JSON   : {out_json}\")\n",
    "print(f\"ðŸ“ Models dir  : {mdl_dir}\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "print(\"\\nðŸ“‹ QUICK SUMMARY (sorted):\")\n",
    "cols_show = [c for c in [\"task\",\"target\",\"horizon\",\"horizon_days\",\"R2\",\"IC\",\"DirAcc\",\"Sharpe\",\"acc\",\"balanced_acc\",\"macro_f1\",\"n_train\",\"n_test\"] if c in results_df.columns]\n",
    "print(results_df[cols_show].sort_values([\"task\",\"target\",\"horizon_days\"]).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0aff9fb-75d5-4037-89bd-1c5b95993551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rebuilt masks for Cell 8 (from dates):\n",
      "   Train: 2013 | Val: 250 | Test: 493\n",
      "   idx range: 2015-01-05 â†’ 2025-12-17\n",
      "   train_end=2022-12-31 | val_end=2023-12-31\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# âœ… FIX FOR CELL 8: build masks from TEMPORAL_SPLITS dates (no train_mask needed)\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "assert \"features_df\" in globals() and features_df is not None, \"features_df missing\"\n",
    "assert \"TEMPORAL_SPLITS\" in globals() and isinstance(TEMPORAL_SPLITS, dict), \"TEMPORAL_SPLITS missing\"\n",
    "\n",
    "idx = pd.to_datetime(features_df.index)\n",
    "\n",
    "train_end = pd.to_datetime(TEMPORAL_SPLITS.get(\"train_end\", \"2022-12-31\"))\n",
    "val_end   = pd.to_datetime(TEMPORAL_SPLITS.get(\"val_end\",   \"2023-12-31\"))\n",
    "\n",
    "train_mask = (idx <= train_end)\n",
    "val_mask   = (idx > train_end) & (idx <= val_end)\n",
    "test_mask  = (idx > val_end)\n",
    "\n",
    "# sanity\n",
    "print(\"âœ… Rebuilt masks for Cell 8 (from dates):\")\n",
    "print(f\"   Train: {train_mask.sum()} | Val: {val_mask.sum()} | Test: {test_mask.sum()}\")\n",
    "print(f\"   idx range: {idx.min().date()} â†’ {idx.max().date()}\")\n",
    "print(f\"   train_end={train_end.date()} | val_end={val_end.date()}\")\n",
    "\n",
    "# (optional) expose for later cells\n",
    "TEMPORAL_SPLITS[\"train_mask\"] = train_mask\n",
    "TEMPORAL_SPLITS[\"val_mask\"]   = val_mask\n",
    "TEMPORAL_SPLITS[\"test_mask\"]  = test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d851f6f0-4352-45af-9821-b60e095b9c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================================\n",
      "ðŸ§¬ CELL 8: FAST MULTI-ASSET SEQUENCE DATASET BUILDER (NO LEAKAGE) [FIXED]\n",
      "=====================================================================================\n",
      "\n",
      "ðŸ”Ž ASSET AVAILABILITY CHECK:\n",
      "   â€¢ In features_df: 10 assets\n",
      "   â€¢ In targets_df : 10 assets\n",
      "   â€¢ In BOTH       : 10 assets\n",
      "\n",
      "âœ… Using ASSET_UNIVERSE (filtered): 10 assets\n",
      "    ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOGL', 'TSLA', 'META', 'JPM', 'KO', 'DIS']\n",
      "\n",
      "âš™ï¸ CONFIG:\n",
      "   â€¢ LOOKBACK: 60\n",
      "   â€¢ STRIDE  : 1\n",
      "   â€¢ SCALE   : True\n",
      "   â€¢ Assets  : 10\n",
      "\n",
      "ðŸ“… SPLITS:\n",
      "   â€¢ Train days: 2013\n",
      "   â€¢ Val days  : 250\n",
      "   â€¢ Test days : 493\n",
      "\n",
      "ðŸ”§ BUILDING SEQUENCES FOR ALL HORIZONS...\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ Horizon: short_1d (1 days)\n",
      "---------------------------------------------------------------------------\n",
      "   â€¢ AAPL      : 2,656 sequences\n",
      "   â€¢ MSFT      : 2,656 sequences\n",
      "   â€¢ NVDA      : 2,656 sequences\n",
      "   â€¢ AMZN      : 2,656 sequences\n",
      "   â€¢ GOOGL     : 2,656 sequences\n",
      "   â€¢ TSLA      : 2,656 sequences\n",
      "   â€¢ META      : 2,656 sequences\n",
      "   â€¢ JPM       : 2,656 sequences\n",
      "   â€¢ KO        : 2,656 sequences\n",
      "   â€¢ DIS       : 2,656 sequences\n",
      "\n",
      "âœ… FINAL SHAPES:\n",
      "   X_train: (19130, 60, 44) | y_train: (19130, 1)\n",
      "   X_val  : (2500, 60, 44) | y_val  : (2500, 1)\n",
      "   X_test : (4930, 60, 44) | y_test : (4930, 1)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ Horizon: short_3d (3 days)\n",
      "---------------------------------------------------------------------------\n",
      "   â€¢ AAPL      : 2,656 sequences\n",
      "   â€¢ MSFT      : 2,656 sequences\n",
      "   â€¢ NVDA      : 2,656 sequences\n",
      "   â€¢ AMZN      : 2,656 sequences\n",
      "   â€¢ GOOGL     : 2,656 sequences\n",
      "   â€¢ TSLA      : 2,656 sequences\n",
      "   â€¢ META      : 2,656 sequences\n",
      "   â€¢ JPM       : 2,656 sequences\n",
      "   â€¢ KO        : 2,656 sequences\n",
      "   â€¢ DIS       : 2,656 sequences\n",
      "\n",
      "âœ… FINAL SHAPES:\n",
      "   X_train: (19130, 60, 44) | y_train: (19130, 1)\n",
      "   X_val  : (2500, 60, 44) | y_val  : (2500, 1)\n",
      "   X_test : (4930, 60, 44) | y_test : (4930, 1)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ Horizon: short_5d (5 days)\n",
      "---------------------------------------------------------------------------\n",
      "   â€¢ AAPL      : 2,656 sequences\n",
      "   â€¢ MSFT      : 2,656 sequences\n",
      "   â€¢ NVDA      : 2,656 sequences\n",
      "   â€¢ AMZN      : 2,656 sequences\n",
      "   â€¢ GOOGL     : 2,656 sequences\n",
      "   â€¢ TSLA      : 2,656 sequences\n",
      "   â€¢ META      : 2,656 sequences\n",
      "   â€¢ JPM       : 2,656 sequences\n",
      "   â€¢ KO        : 2,656 sequences\n",
      "   â€¢ DIS       : 2,656 sequences\n",
      "\n",
      "âœ… FINAL SHAPES:\n",
      "   X_train: (19130, 60, 44) | y_train: (19130, 1)\n",
      "   X_val  : (2500, 60, 44) | y_val  : (2500, 1)\n",
      "   X_test : (4930, 60, 44) | y_test : (4930, 1)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ Horizon: intermediate_10d (10 days)\n",
      "---------------------------------------------------------------------------\n",
      "   â€¢ AAPL      : 2,656 sequences\n",
      "   â€¢ MSFT      : 2,656 sequences\n",
      "   â€¢ NVDA      : 2,656 sequences\n",
      "   â€¢ AMZN      : 2,656 sequences\n",
      "   â€¢ GOOGL     : 2,656 sequences\n",
      "   â€¢ TSLA      : 2,656 sequences\n",
      "   â€¢ META      : 2,656 sequences\n",
      "   â€¢ JPM       : 2,656 sequences\n",
      "   â€¢ KO        : 2,656 sequences\n",
      "   â€¢ DIS       : 2,656 sequences\n",
      "\n",
      "âœ… FINAL SHAPES:\n",
      "   X_train: (19130, 60, 44) | y_train: (19130, 1)\n",
      "   X_val  : (2500, 60, 44) | y_val  : (2500, 1)\n",
      "   X_test : (4930, 60, 44) | y_test : (4930, 1)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ Horizon: intermediate_15d (15 days)\n",
      "---------------------------------------------------------------------------\n",
      "   â€¢ AAPL      : 2,656 sequences\n",
      "   â€¢ MSFT      : 2,656 sequences\n",
      "   â€¢ NVDA      : 2,656 sequences\n",
      "   â€¢ AMZN      : 2,656 sequences\n",
      "   â€¢ GOOGL     : 2,656 sequences\n",
      "   â€¢ TSLA      : 2,656 sequences\n",
      "   â€¢ META      : 2,656 sequences\n",
      "   â€¢ JPM       : 2,656 sequences\n",
      "   â€¢ KO        : 2,656 sequences\n",
      "   â€¢ DIS       : 2,656 sequences\n",
      "\n",
      "âœ… FINAL SHAPES:\n",
      "   X_train: (19130, 60, 44) | y_train: (19130, 1)\n",
      "   X_val  : (2500, 60, 44) | y_val  : (2500, 1)\n",
      "   X_test : (4930, 60, 44) | y_test : (4930, 1)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ Horizon: intermediate_20d (20 days)\n",
      "---------------------------------------------------------------------------\n",
      "   â€¢ AAPL      : 2,656 sequences\n",
      "   â€¢ MSFT      : 2,656 sequences\n",
      "   â€¢ NVDA      : 2,656 sequences\n",
      "   â€¢ AMZN      : 2,656 sequences\n",
      "   â€¢ GOOGL     : 2,656 sequences\n",
      "   â€¢ TSLA      : 2,656 sequences\n",
      "   â€¢ META      : 2,656 sequences\n",
      "   â€¢ JPM       : 2,656 sequences\n",
      "   â€¢ KO        : 2,656 sequences\n",
      "   â€¢ DIS       : 2,656 sequences\n",
      "\n",
      "âœ… FINAL SHAPES:\n",
      "   X_train: (19130, 60, 44) | y_train: (19130, 1)\n",
      "   X_val  : (2500, 60, 44) | y_val  : (2500, 1)\n",
      "   X_test : (4930, 60, 44) | y_test : (4930, 1)\n",
      "\n",
      "ðŸ’¾ SAVING NPZ DATASETS...\n",
      "   âœ“ short_1d: /workspace/quantgenius_project/data/seq_short_1d_20260117_145029.npz\n",
      "   âœ“ short_3d: /workspace/quantgenius_project/data/seq_short_3d_20260117_145029.npz\n",
      "   âœ“ short_5d: /workspace/quantgenius_project/data/seq_short_5d_20260117_145029.npz\n",
      "   âœ“ intermediate_10d: /workspace/quantgenius_project/data/seq_intermediate_10d_20260117_145029.npz\n",
      "   âœ“ intermediate_15d: /workspace/quantgenius_project/data/seq_intermediate_15d_20260117_145029.npz\n",
      "   âœ“ intermediate_20d: /workspace/quantgenius_project/data/seq_intermediate_20d_20260117_145029.npz\n",
      "\n",
      "ðŸ“ Saved stats : /workspace/quantgenius_project/tables/cell8_seq_stats_20260117_145029.csv\n",
      "ðŸ“ Saved meta  : /workspace/quantgenius_project/tables/cell8_seq_meta_20260117_145029.json\n",
      "\n",
      "=====================================================================================\n",
      "âœ… CELL 8 COMPLETE: Sequence datasets built + saved (NPZ)\n",
      "âž¡ï¸ NEXT: CELL 9 (LSTM Training on GPU)\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CELL 8: Multi-Asset Sequence Dataset Builder (FAST + NO LEAKAGE) â€” FIXED (FOREX SAFE)\n",
    "# ================================\n",
    "# Fixes applied:\n",
    "# âœ… Filters ASSET_UNIVERSE to assets that exist in BOTH features_df and targets_df (prevents KeyError like 'EURUSD=X')\n",
    "# âœ… Per-asset guard inside loop (extra safety)\n",
    "# âœ… Robust split-by-dates using pandas Index.intersection (faster + safer than set membership)\n",
    "# âœ… Saves scaler params (mean/std) in meta so Vast.ai can reproduce inference without pickling sklearn objects\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"ðŸ§¬ CELL 8: FAST MULTI-ASSET SEQUENCE DATASET BUILDER (NO LEAKAGE) [FIXED]\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "# ---------\n",
    "# 0) Sanity checks\n",
    "# ---------\n",
    "required = [\"features_df\", \"targets_df\", \"FORECAST_HORIZONS\", \"TEMPORAL_SPLITS\",\n",
    "            \"PROJECT_DIRS\", \"ASSET_UNIVERSE\", \"STOCKS\", \"FX_PAIRS\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing globals: {missing}. Run Cells 1â€“7 first.\")\n",
    "\n",
    "if not isinstance(features_df.columns, pd.MultiIndex):\n",
    "    raise ValueError(\"features_df.columns must be a MultiIndex of (asset, feature).\")\n",
    "\n",
    "if not isinstance(targets_df.columns, pd.MultiIndex):\n",
    "    raise ValueError(\"targets_df.columns must be a MultiIndex of (asset, target_name).\")\n",
    "\n",
    "# ---------\n",
    "# 0.5) FIX: Filter ASSET_UNIVERSE to assets available in BOTH features_df and targets_df\n",
    "# ---------\n",
    "feat_assets = set(features_df.columns.get_level_values(0))\n",
    "tgt_assets  = set(targets_df.columns.get_level_values(0))\n",
    "available_assets = sorted(list(feat_assets.intersection(tgt_assets)))\n",
    "\n",
    "missing_assets = [a for a in ASSET_UNIVERSE if a not in available_assets]\n",
    "print(\"\\nðŸ”Ž ASSET AVAILABILITY CHECK:\")\n",
    "print(f\"   â€¢ In features_df: {len(feat_assets)} assets\")\n",
    "print(f\"   â€¢ In targets_df : {len(tgt_assets)} assets\")\n",
    "print(f\"   â€¢ In BOTH       : {len(available_assets)} assets\")\n",
    "\n",
    "if missing_assets:\n",
    "    print(\"\\nâš ï¸ These assets are in ASSET_UNIVERSE but missing in features/targets and will be SKIPPED:\")\n",
    "    print(\"   \", missing_assets)\n",
    "\n",
    "ASSET_UNIVERSE = [a for a in ASSET_UNIVERSE if a in available_assets]\n",
    "print(f\"\\nâœ… Using ASSET_UNIVERSE (filtered): {len(ASSET_UNIVERSE)} assets\")\n",
    "print(\"   \", ASSET_UNIVERSE)\n",
    "\n",
    "# ---------\n",
    "# 1) Config (tune these for speed)\n",
    "# ---------\n",
    "LOOKBACK = 60          # 60 trading days\n",
    "STRIDE = 1             # 1 = daily sliding\n",
    "MIN_SEQ_PER_ASSET = 50 # skip assets with too little sequence data\n",
    "SCALE = True           # fit scaler on TRAIN ONLY per horizon (no leakage)\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\nâš™ï¸ CONFIG:\")\n",
    "print(f\"   â€¢ LOOKBACK: {LOOKBACK}\")\n",
    "print(f\"   â€¢ STRIDE  : {STRIDE}\")\n",
    "print(f\"   â€¢ SCALE   : {SCALE}\")\n",
    "print(f\"   â€¢ Assets  : {len(ASSET_UNIVERSE)}\")\n",
    "\n",
    "# ---------\n",
    "# 2) Prepare split masks / dates (strict temporal)\n",
    "# ---------\n",
    "idx = features_df.index\n",
    "train_mask = TEMPORAL_SPLITS[\"train_mask\"]\n",
    "val_mask   = TEMPORAL_SPLITS[\"val_mask\"]\n",
    "test_mask  = TEMPORAL_SPLITS[\"test_mask\"]\n",
    "\n",
    "train_idx = idx[train_mask]\n",
    "val_idx   = idx[val_mask]\n",
    "test_idx  = idx[test_mask]\n",
    "\n",
    "print(\"\\nðŸ“… SPLITS:\")\n",
    "print(f\"   â€¢ Train days: {len(train_idx)}\")\n",
    "print(f\"   â€¢ Val days  : {len(val_idx)}\")\n",
    "print(f\"   â€¢ Test days : {len(test_idx)}\")\n",
    "\n",
    "# ---------\n",
    "# 3) Helper: asset feature extraction + asset-ID features\n",
    "# ---------\n",
    "def get_asset_features(asset: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract one asset's feature block: features_df columns MultiIndex (asset, feature) -> DataFrame[feature].\"\"\"\n",
    "    X = features_df.loc[:, asset].copy()  # drops first level\n",
    "    # Add asset-type flags (constant across rows)\n",
    "    X[\"asset_is_stock\"] = 1 if asset in STOCKS else 0\n",
    "    X[\"asset_is_forex\"] = 1 if asset in FX_PAIRS else 0\n",
    "    return X\n",
    "\n",
    "def get_asset_target(asset: str, horizon_key: str) -> pd.Series:\n",
    "    \"\"\"Target column format: (asset, f'y_{horizon_key}')\"\"\"\n",
    "    col = (asset, f\"y_{horizon_key}\")\n",
    "    return targets_df[col]\n",
    "\n",
    "# ---------\n",
    "# 4) Fast sequence creation (numpy)\n",
    "# ---------\n",
    "def make_sequences_fast(X: np.ndarray, y: np.ndarray, dates: np.ndarray, lookback: int, stride: int):\n",
    "    \"\"\"\n",
    "    Create sequences using sliding windows:\n",
    "    - X_seq shape: (n_seq, lookback, n_features)\n",
    "    - y_seq shape: (n_seq, 1) where target is aligned with window end index\n",
    "    - d_seq: end-date for each sequence\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    if n <= lookback:\n",
    "        return None\n",
    "\n",
    "    # (n-lookback+1, lookback, n_features)\n",
    "    Xw = sliding_window_view(X, window_shape=(lookback, X.shape[1]))[:, 0, :, :]\n",
    "    yw = y[lookback - 1:]\n",
    "    dw = dates[lookback - 1:]\n",
    "\n",
    "    # stride\n",
    "    Xw = Xw[::stride]\n",
    "    yw = yw[::stride]\n",
    "    dw = dw[::stride]\n",
    "\n",
    "    # drop windows with NaNs (fast)\n",
    "    flat = Xw.reshape(Xw.shape[0], -1)\n",
    "    good = (~np.isnan(flat).any(axis=1)) & (~np.isnan(yw))\n",
    "    if good.sum() == 0:\n",
    "        return None\n",
    "\n",
    "    Xw = Xw[good].astype(np.float32)\n",
    "    yw = yw[good].astype(np.float32).reshape(-1, 1)\n",
    "    dw = dw[good]\n",
    "    return Xw, yw, dw\n",
    "\n",
    "def split_by_dates_vectorized(X_seq, y_seq, d_seq, train_idx, val_idx, test_idx):\n",
    "    d_seq = pd.to_datetime(d_seq)\n",
    "    d_seq = pd.DatetimeIndex(d_seq)\n",
    "\n",
    "    tr = d_seq.isin(train_idx)\n",
    "    va = d_seq.isin(val_idx)\n",
    "    te = d_seq.isin(test_idx)\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_seq[tr], \"y_train\": y_seq[tr],\n",
    "        \"X_val\":   X_seq[va], \"y_val\":   y_seq[va],\n",
    "        \"X_test\":  X_seq[te], \"y_test\":  y_seq[te],\n",
    "        \"n_train\": int(tr.sum()), \"n_val\": int(va.sum()), \"n_test\": int(te.sum()),\n",
    "        \"d_train\": d_seq[tr].astype(str).tolist(),\n",
    "        \"d_val\":   d_seq[va].astype(str).tolist(),\n",
    "        \"d_test\":  d_seq[te].astype(str).tolist(),\n",
    "    }\n",
    "\n",
    "# ---------\n",
    "# 5) Build SEQ_DATASETS for ALL horizons\n",
    "# ---------\n",
    "SEQ_DATASETS = {}\n",
    "stats_rows = []\n",
    "\n",
    "print(\"\\nðŸ”§ BUILDING SEQUENCES FOR ALL HORIZONS...\")\n",
    "\n",
    "for horizon_key, h_days in FORECAST_HORIZONS.items():\n",
    "    print(\"\\n\" + \"-\" * 75)\n",
    "    print(f\"ðŸ Horizon: {horizon_key} ({h_days} days)\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    X_all_list, y_all_list, d_all_list, asset_id_list = [], [], [], []\n",
    "    feature_dim_ref = None\n",
    "\n",
    "    for asset_id, asset in enumerate(ASSET_UNIVERSE):\n",
    "\n",
    "        # Extra guard (prevents KeyError even if universe changes later)\n",
    "        if asset not in feat_assets:\n",
    "            print(f\"   âš ï¸ {asset:10s}: not in features_df, skipping\")\n",
    "            continue\n",
    "        tgt_col = (asset, f\"y_{horizon_key}\")\n",
    "        if tgt_col not in targets_df.columns:\n",
    "            print(f\"   âš ï¸ {asset:10s}: missing target {tgt_col}, skipping\")\n",
    "            continue\n",
    "\n",
    "        X_df = get_asset_features(asset)\n",
    "        y_s  = get_asset_target(asset, horizon_key)\n",
    "\n",
    "        # Align (inner join) and ensure we keep the global index order\n",
    "        aligned = X_df.join(y_s.rename(\"y\"), how=\"inner\")\n",
    "        aligned = aligned.reindex(idx)  # enforce global timeline index\n",
    "        dates = aligned.index.values\n",
    "\n",
    "        X = aligned.drop(columns=[\"y\"]).to_numpy(dtype=np.float32)\n",
    "        y = aligned[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "        out = make_sequences_fast(X, y, dates, LOOKBACK, STRIDE)\n",
    "        if out is None:\n",
    "            print(f\"   âš ï¸ {asset:10s}: no valid sequences (NaNs / too short), skipping\")\n",
    "            continue\n",
    "\n",
    "        X_seq, y_seq, d_seq = out\n",
    "        if len(X_seq) < MIN_SEQ_PER_ASSET:\n",
    "            print(f\"   âš ï¸ {asset:10s}: only {len(X_seq)} sequences (<{MIN_SEQ_PER_ASSET}), skipping\")\n",
    "            continue\n",
    "\n",
    "        if feature_dim_ref is None:\n",
    "            feature_dim_ref = X_seq.shape[2]\n",
    "\n",
    "        X_all_list.append(X_seq)\n",
    "        y_all_list.append(y_seq)\n",
    "        d_all_list.append(d_seq)\n",
    "        asset_id_list.append(np.full((len(X_seq),), asset_id, dtype=np.int64))\n",
    "\n",
    "        print(f\"   â€¢ {asset:10s}: {len(X_seq):,} sequences\")\n",
    "\n",
    "    if not X_all_list:\n",
    "        print(\"   âŒ No sequences created for this horizon.\")\n",
    "        continue\n",
    "\n",
    "    X_all = np.concatenate(X_all_list, axis=0)\n",
    "    y_all = np.concatenate(y_all_list, axis=0)\n",
    "    d_all = np.concatenate(d_all_list, axis=0)\n",
    "    asset_ids = np.concatenate(asset_id_list, axis=0)\n",
    "\n",
    "    # Split by end-date (strict temporal)\n",
    "    split = split_by_dates_vectorized(X_all, y_all, d_all, train_idx, val_idx, test_idx)\n",
    "    if split[\"n_train\"] == 0:\n",
    "        print(\"   âŒ No training sequences after split â€” skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Scale features (fit on TRAIN only) â€” no leakage\n",
    "    scaler = None\n",
    "    scaler_params = None\n",
    "    if SCALE:\n",
    "        X_tr = split[\"X_train\"]\n",
    "        n_tr, L, F = X_tr.shape\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_tr.reshape(-1, F))\n",
    "\n",
    "        # Save params so you can recreate scaler on Vast.ai without pickling objects\n",
    "        scaler_params = {\n",
    "            \"mean_\": scaler.mean_.tolist(),\n",
    "            \"scale_\": scaler.scale_.tolist(),\n",
    "            \"var_\": scaler.var_.tolist() if hasattr(scaler, \"var_\") else None,\n",
    "            \"n_features\": int(F),\n",
    "        }\n",
    "\n",
    "        def _scale(X):\n",
    "            if X is None or len(X) == 0:\n",
    "                return X\n",
    "            n, L, F = X.shape\n",
    "            return scaler.transform(X.reshape(-1, F)).reshape(n, L, F).astype(np.float32)\n",
    "\n",
    "        split[\"X_train\"] = _scale(split[\"X_train\"])\n",
    "        split[\"X_val\"]   = _scale(split[\"X_val\"])\n",
    "        split[\"X_test\"]  = _scale(split[\"X_test\"])\n",
    "\n",
    "    # Also split asset_ids by date masks (use the same d_all as used in split)\n",
    "    d_all_idx = pd.DatetimeIndex(pd.to_datetime(d_all))\n",
    "    tr_mask = d_all_idx.isin(train_idx)\n",
    "    va_mask = d_all_idx.isin(val_idx)\n",
    "    te_mask = d_all_idx.isin(test_idx)\n",
    "\n",
    "    SEQ_DATASETS[horizon_key] = {\n",
    "        \"X_train\": split[\"X_train\"], \"y_train\": split[\"y_train\"], \"asset_ids_train\": asset_ids[tr_mask],\n",
    "        \"X_val\":   split[\"X_val\"],   \"y_val\":   split[\"y_val\"],   \"asset_ids_val\":   asset_ids[va_mask],\n",
    "        \"X_test\":  split[\"X_test\"],  \"y_test\":  split[\"y_test\"],  \"asset_ids_test\":  asset_ids[te_mask],\n",
    "        \"d_train\": split[\"d_train\"], \"d_val\": split[\"d_val\"], \"d_test\": split[\"d_test\"],\n",
    "        \"lookback\": int(LOOKBACK),\n",
    "        \"n_features\": int(feature_dim_ref),\n",
    "        \"n_assets\": int(len(ASSET_UNIVERSE)),\n",
    "        \"horizon_days\": int(h_days),\n",
    "        \"scaler_params\": scaler_params,\n",
    "    }\n",
    "\n",
    "    stats_rows.append({\n",
    "        \"horizon\": horizon_key,\n",
    "        \"horizon_days\": int(h_days),\n",
    "        \"n_total\": int(len(X_all)),\n",
    "        \"n_train\": split[\"n_train\"],\n",
    "        \"n_val\": split[\"n_val\"],\n",
    "        \"n_test\": split[\"n_test\"],\n",
    "        \"lookback\": int(LOOKBACK),\n",
    "        \"n_features\": int(feature_dim_ref),\n",
    "        \"n_assets\": int(len(ASSET_UNIVERSE)),\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… FINAL SHAPES:\")\n",
    "    print(f\"   X_train: {SEQ_DATASETS[horizon_key]['X_train'].shape} | y_train: {SEQ_DATASETS[horizon_key]['y_train'].shape}\")\n",
    "    print(f\"   X_val  : {SEQ_DATASETS[horizon_key]['X_val'].shape} | y_val  : {SEQ_DATASETS[horizon_key]['y_val'].shape}\")\n",
    "    print(f\"   X_test : {SEQ_DATASETS[horizon_key]['X_test'].shape} | y_test : {SEQ_DATASETS[horizon_key]['y_test'].shape}\")\n",
    "\n",
    "# ---------\n",
    "# 6) Save datasets (NPZ per horizon) + stats\n",
    "# ---------\n",
    "print(\"\\nðŸ’¾ SAVING NPZ DATASETS...\")\n",
    "\n",
    "stats_df = pd.DataFrame(stats_rows)\n",
    "stats_path = PROJECT_DIRS[\"tables\"] / f\"cell8_seq_stats_{stamp}.csv\"\n",
    "stats_df.to_csv(stats_path, index=False)\n",
    "\n",
    "saved_files = {}\n",
    "for horizon_key, ds in SEQ_DATASETS.items():\n",
    "    out_path = PROJECT_DIRS[\"data\"] / f\"seq_{horizon_key}_{stamp}.npz\"\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        X_train=ds[\"X_train\"], y_train=ds[\"y_train\"], asset_ids_train=ds[\"asset_ids_train\"],\n",
    "        X_val=ds[\"X_val\"], y_val=ds[\"y_val\"], asset_ids_val=ds[\"asset_ids_val\"],\n",
    "        X_test=ds[\"X_test\"], y_test=ds[\"y_test\"], asset_ids_test=ds[\"asset_ids_test\"],\n",
    "        lookback=np.array([ds[\"lookback\"]], dtype=np.int32),\n",
    "        horizon_days=np.array([ds[\"horizon_days\"]], dtype=np.int32),\n",
    "    )\n",
    "    saved_files[horizon_key] = str(out_path)\n",
    "    print(f\"   âœ“ {horizon_key}: {out_path}\")\n",
    "\n",
    "meta_path = PROJECT_DIRS[\"tables\"] / f\"cell8_seq_meta_{stamp}.json\"\n",
    "meta = {\n",
    "    \"created_at\": stamp,\n",
    "    \"lookback\": int(LOOKBACK),\n",
    "    \"stride\": int(STRIDE),\n",
    "    \"scale_train_only\": bool(SCALE),\n",
    "    \"assets_used\": ASSET_UNIVERSE,\n",
    "    \"assets_skipped\": missing_assets,\n",
    "    \"horizons\": {k: int(v) for k, v in FORECAST_HORIZONS.items()},\n",
    "    \"files_npz\": saved_files,\n",
    "    \"stats_csv\": str(stats_path),\n",
    "    \"scaler_params_per_horizon\": {k: SEQ_DATASETS[k].get(\"scaler_params\") for k in SEQ_DATASETS.keys()},\n",
    "}\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"\\nðŸ“ Saved stats : {stats_path}\")\n",
    "print(f\"ðŸ“ Saved meta  : {meta_path}\")\n",
    "\n",
    "globals().update({\n",
    "    \"SEQ_DATASETS\": SEQ_DATASETS,\n",
    "    \"SEQ_STATS_DF\": stats_df,\n",
    "    \"CELL8_META_PATH\": str(meta_path),\n",
    "    \"CELL8_SAVED_FILES\": saved_files,\n",
    "    \"LOOKBACK_WINDOW\": int(LOOKBACK),\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"âœ… CELL 8 COMPLETE: Sequence datasets built + saved (NPZ)\")\n",
    "print(\"âž¡ï¸ NEXT: CELL 9 (LSTM Training on GPU)\")\n",
    "print(\"=\" * 85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de1e23a-7db5-4858-b76c-5629da4db39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "ðŸ§  CELL 9 (UPDATED v3): LSTM TRAINING (TARGET-NORM + HUBER + CLIP + SCHEDULER)\n",
      "==========================================================================================\n",
      "ðŸ–¥ï¸ Device: cuda\n",
      "\n",
      "âš™ï¸ TRAINING CONFIG:\n",
      "   â€¢ Batch: 256\n",
      "   â€¢ Epochs: 50\n",
      "   â€¢ LR: 0.001\n",
      "   â€¢ Hidden: 128 | Layers: 2 | Dropout: 0.2\n",
      "   â€¢ Asset Emb Dim: 8\n",
      "   â€¢ Target Norm (train-only): True\n",
      "   â€¢ Loss: Huber | delta=1.0\n",
      "   â€¢ Grad clip: 1.0\n",
      "   â€¢ Scheduler: ReduceLROnPlateau\n",
      "   â€¢ AMP: True\n",
      "\n",
      "ðŸš€ TRAINING LSTM PER HORIZON...\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: short_1d (1d)\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.339044 | ValLoss 0.303927 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.335934 | ValLoss 0.304687 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 03 | TrainLoss 0.335402 | ValLoss 0.304436 | LR 1.00e-03 | Pat 2/8\n",
      "   Epoch 04 | TrainLoss 0.335501 | ValLoss 0.304112 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 05 | TrainLoss 0.334576 | ValLoss 0.304430 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 06 | TrainLoss 0.333865 | ValLoss 0.304115 | LR 5.00e-04 | Pat 5/8\n",
      "   Epoch 07 | TrainLoss 0.332323 | ValLoss 0.303865 | LR 5.00e-04 | Pat 0/8\n",
      "   Epoch 08 | TrainLoss 0.329791 | ValLoss 0.303757 | LR 5.00e-04 | Pat 0/8\n",
      "   Epoch 09 | TrainLoss 0.327687 | ValLoss 0.302978 | LR 5.00e-04 | Pat 0/8\n",
      "   Epoch 10 | TrainLoss 0.324719 | ValLoss 0.302712 | LR 5.00e-04 | Pat 0/8\n",
      "   Epoch 11 | TrainLoss 0.322253 | ValLoss 0.302660 | LR 5.00e-04 | Pat 0/8\n",
      "   Epoch 12 | TrainLoss 0.322527 | ValLoss 0.301630 | LR 5.00e-04 | Pat 0/8\n",
      "   Epoch 13 | TrainLoss 0.319157 | ValLoss 0.304811 | LR 5.00e-04 | Pat 1/8\n",
      "   Epoch 14 | TrainLoss 0.317045 | ValLoss 0.304428 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 15 | TrainLoss 0.316906 | ValLoss 0.304590 | LR 2.50e-04 | Pat 3/8\n",
      "   Epoch 16 | TrainLoss 0.312303 | ValLoss 0.304402 | LR 2.50e-04 | Pat 4/8\n",
      "   Epoch 17 | TrainLoss 0.311627 | ValLoss 0.305857 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 18 | TrainLoss 0.309639 | ValLoss 0.304898 | LR 1.25e-04 | Pat 6/8\n",
      "   Epoch 19 | TrainLoss 0.308856 | ValLoss 0.305892 | LR 1.25e-04 | Pat 7/8\n",
      "   Epoch 20 | TrainLoss 0.307046 | ValLoss 0.306302 | LR 1.25e-04 | Pat 8/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | loss(norm) 0.328303 | RMSE 0.022458 | MAE 0.014613 | RÂ² 0.0062 | DirAcc 0.507\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: short_3d (3d)\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.353387 | ValLoss 0.322866 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.349159 | ValLoss 0.326818 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 03 | TrainLoss 0.346879 | ValLoss 0.327566 | LR 1.00e-03 | Pat 2/8\n",
      "   Epoch 04 | TrainLoss 0.341519 | ValLoss 0.323327 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 05 | TrainLoss 0.333676 | ValLoss 0.333185 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 06 | TrainLoss 0.325892 | ValLoss 0.333980 | LR 5.00e-04 | Pat 5/8\n",
      "   Epoch 07 | TrainLoss 0.320350 | ValLoss 0.335793 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 08 | TrainLoss 0.313566 | ValLoss 0.342419 | LR 2.50e-04 | Pat 7/8\n",
      "   Epoch 09 | TrainLoss 0.310080 | ValLoss 0.344560 | LR 2.50e-04 | Pat 8/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | loss(norm) 0.353126 | RMSE 0.038270 | MAE 0.026053 | RÂ² 0.0096 | DirAcc 0.554\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: short_5d (5d)\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.354778 | ValLoss 0.339791 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.347205 | ValLoss 0.342352 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 03 | TrainLoss 0.337180 | ValLoss 0.343556 | LR 1.00e-03 | Pat 2/8\n",
      "   Epoch 04 | TrainLoss 0.324997 | ValLoss 0.366158 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 05 | TrainLoss 0.306593 | ValLoss 0.358516 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 06 | TrainLoss 0.295394 | ValLoss 0.368243 | LR 5.00e-04 | Pat 5/8\n",
      "   Epoch 07 | TrainLoss 0.288221 | ValLoss 0.388095 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 08 | TrainLoss 0.276568 | ValLoss 0.391882 | LR 2.50e-04 | Pat 7/8\n",
      "   Epoch 09 | TrainLoss 0.269332 | ValLoss 0.403011 | LR 2.50e-04 | Pat 8/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | loss(norm) 0.359234 | RMSE 0.048713 | MAE 0.034302 | RÂ² -0.0029 | DirAcc 0.534\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: intermediate_10d (10d)\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.358498 | ValLoss 0.356539 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.337703 | ValLoss 0.359935 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 03 | TrainLoss 0.318220 | ValLoss 0.408709 | LR 1.00e-03 | Pat 2/8\n",
      "   Epoch 04 | TrainLoss 0.292635 | ValLoss 0.396640 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 05 | TrainLoss 0.264893 | ValLoss 0.415752 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 06 | TrainLoss 0.248274 | ValLoss 0.431691 | LR 5.00e-04 | Pat 5/8\n",
      "   Epoch 07 | TrainLoss 0.234442 | ValLoss 0.432635 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 08 | TrainLoss 0.218517 | ValLoss 0.459340 | LR 2.50e-04 | Pat 7/8\n",
      "   Epoch 09 | TrainLoss 0.211345 | ValLoss 0.453288 | LR 2.50e-04 | Pat 8/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | loss(norm) 0.370668 | RMSE 0.068314 | MAE 0.048905 | RÂ² -0.0216 | DirAcc 0.541\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: intermediate_15d (15d)\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.351970 | ValLoss 0.352790 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.326457 | ValLoss 0.378000 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 03 | TrainLoss 0.293268 | ValLoss 0.408517 | LR 1.00e-03 | Pat 2/8\n",
      "   Epoch 04 | TrainLoss 0.263241 | ValLoss 0.444592 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 05 | TrainLoss 0.229423 | ValLoss 0.436975 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 06 | TrainLoss 0.211699 | ValLoss 0.502412 | LR 5.00e-04 | Pat 5/8\n",
      "   Epoch 07 | TrainLoss 0.197415 | ValLoss 0.517431 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 08 | TrainLoss 0.180136 | ValLoss 0.532186 | LR 2.50e-04 | Pat 7/8\n",
      "   Epoch 09 | TrainLoss 0.174119 | ValLoss 0.552483 | LR 2.50e-04 | Pat 8/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | loss(norm) 0.362985 | RMSE 0.082496 | MAE 0.059844 | RÂ² -0.0007 | DirAcc 0.560\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: intermediate_20d (20d)\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.350483 | ValLoss 0.343849 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.314394 | ValLoss 0.353961 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 03 | TrainLoss 0.281775 | ValLoss 0.382370 | LR 1.00e-03 | Pat 2/8\n",
      "   Epoch 04 | TrainLoss 0.249330 | ValLoss 0.386533 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 05 | TrainLoss 0.215241 | ValLoss 0.381707 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 06 | TrainLoss 0.195978 | ValLoss 0.388078 | LR 5.00e-04 | Pat 5/8\n",
      "   Epoch 07 | TrainLoss 0.182597 | ValLoss 0.409459 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 08 | TrainLoss 0.165443 | ValLoss 0.407402 | LR 2.50e-04 | Pat 7/8\n",
      "   Epoch 09 | TrainLoss 0.156649 | ValLoss 0.416056 | LR 2.50e-04 | Pat 8/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | loss(norm) 0.356634 | RMSE 0.094008 | MAE 0.069027 | RÂ² 0.0013 | DirAcc 0.559\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "âœ… CELL 9 COMPLETE (UPDATED v3)\n",
      "ðŸ“ Results: /workspace/quantgenius_project/tables/cell9_lstm_results_UPDATEDv3_20260117_145043.csv\n",
      "ðŸ“ Meta   : /workspace/quantgenius_project/tables/cell9_lstm_meta_UPDATEDv3_20260117_145043.json\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CELL 9 (UPDATED v3): LSTM TRAINING (TARGET-NORM + HUBER + CLIP + SCHEDULER)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ðŸ§  CELL 9 (UPDATED v3): LSTM TRAINING (TARGET-NORM + HUBER + CLIP + SCHEDULER)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "import numpy as np, pandas as pd, json, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "required = [\"SEQ_DATASETS\", \"FORECAST_HORIZONS\", \"PROJECT_DIRS\", \"ASSET_UNIVERSE\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing globals: {missing}. Run Cells 1â€“8 first.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "# CONFIG\n",
    "BATCH_SIZE = 256 if device.type == \"cuda\" else 128\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 8\n",
    "\n",
    "HIDDEN = 128\n",
    "LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "ASSET_EMB_DIM = 8\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "CLIP_NORM = 1.0\n",
    "HUBER_DELTA = 1.0\n",
    "TARGET_NORM = True  # train-only normalization per horizon\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_DIR = PROJECT_DIRS.get(\"models\", PROJECT_DIRS[\"tables\"].parent / \"models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nâš™ï¸ TRAINING CONFIG:\")\n",
    "print(f\"   â€¢ Batch: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Epochs: {EPOCHS}\")\n",
    "print(f\"   â€¢ LR: {LR}\")\n",
    "print(f\"   â€¢ Hidden: {HIDDEN} | Layers: {LAYERS} | Dropout: {DROPOUT}\")\n",
    "print(f\"   â€¢ Asset Emb Dim: {ASSET_EMB_DIM}\")\n",
    "print(f\"   â€¢ Target Norm (train-only): {TARGET_NORM}\")\n",
    "print(f\"   â€¢ Loss: Huber | delta={HUBER_DELTA}\")\n",
    "print(f\"   â€¢ Grad clip: {CLIP_NORM}\")\n",
    "print(f\"   â€¢ Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"   â€¢ AMP: {USE_AMP}\")\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, asset_ids):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.asset_ids = torch.tensor(asset_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.asset_ids[idx]\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features, n_assets, asset_emb_dim=8, hidden=128, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.asset_emb = nn.Embedding(n_assets, asset_emb_dim)\n",
    "        in_dim = n_features + asset_emb_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=in_dim,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if layers > 1 else 0.0\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, asset_id):\n",
    "        emb = self.asset_emb(asset_id)                  # (B,E)\n",
    "        emb = emb.unsqueeze(1).repeat(1, x.size(1), 1)  # (B,L,E)\n",
    "        x = torch.cat([x, emb], dim=-1)                 # (B,L,F+E)\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "def rmse_np(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def r2_np(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return 1.0 - ss_res / (ss_tot + 1e-12)\n",
    "\n",
    "def dir_acc(y_true, y_pred):\n",
    "    return float((np.sign(y_true) == np.sign(y_pred)).mean())\n",
    "\n",
    "def run_epoch(model, loader, loss_fn, optimizer=None, amp_scaler=None):\n",
    "    train_mode = optimizer is not None\n",
    "    model.train(train_mode)\n",
    "\n",
    "    losses = []\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for x, y, asset_id in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        asset_id = asset_id.to(device)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            pred = model(x, asset_id)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "        if train_mode:\n",
    "            if USE_AMP:\n",
    "                amp_scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                amp_scaler.step(optimizer)\n",
    "                amp_scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "        losses.append(float(loss.item()))\n",
    "        y_true_all.append(y.detach().cpu().numpy())\n",
    "        y_pred_all.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    return float(np.mean(losses)), np.vstack(y_true_all), np.vstack(y_pred_all)\n",
    "\n",
    "print(\"\\nðŸš€ TRAINING LSTM PER HORIZON...\\n\")\n",
    "\n",
    "results_rows = []\n",
    "saved_models = {}\n",
    "\n",
    "for horizon_key, h_days in FORECAST_HORIZONS.items():\n",
    "    if horizon_key not in SEQ_DATASETS:\n",
    "        print(f\"âš ï¸ Skipping {horizon_key} (no sequences built).\")\n",
    "        continue\n",
    "\n",
    "    ds = SEQ_DATASETS[horizon_key]\n",
    "    X_train, y_train = ds[\"X_train\"], ds[\"y_train\"]\n",
    "    X_val, y_val     = ds[\"X_val\"], ds[\"y_val\"]\n",
    "    X_test, y_test   = ds[\"X_test\"], ds[\"y_test\"]\n",
    "\n",
    "    a_train = ds[\"asset_ids_train\"]\n",
    "    a_val   = ds[\"asset_ids_val\"]\n",
    "    a_test  = ds[\"asset_ids_test\"]\n",
    "\n",
    "    # Target norm (train only)\n",
    "    if TARGET_NORM:\n",
    "        mu = float(np.mean(y_train))\n",
    "        sd = float(np.std(y_train) + 1e-12)\n",
    "        y_train_n = (y_train - mu) / sd\n",
    "        y_val_n   = (y_val   - mu) / sd\n",
    "        y_test_n  = (y_test  - mu) / sd\n",
    "    else:\n",
    "        mu, sd = 0.0, 1.0\n",
    "        y_train_n, y_val_n, y_test_n = y_train, y_val, y_test\n",
    "\n",
    "    n_features = X_train.shape[2]\n",
    "    n_assets = int(ds[\"n_assets\"])\n",
    "\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"ðŸ“Š Horizon: {horizon_key} ({h_days}d)\")\n",
    "    print(f\"   Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(X_train, y_train_n, a_train), batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader   = DataLoader(SeqDataset(X_val,   y_val_n,   a_val),   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(SeqDataset(X_test,  y_test_n,  a_test),  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = LSTMRegressor(n_features=n_features, n_assets=n_assets, asset_emb_dim=ASSET_EMB_DIM,\n",
    "                         hidden=HIDDEN, layers=LAYERS, dropout=DROPOUT).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # âœ… FIX: ReduceLROnPlateau no verbose arg (some torch builds error on it)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=2, threshold=1e-4, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    amp_scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=HUBER_DELTA)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_path = MODEL_DIR / f\"lstm_UPDATEDv3_{horizon_key}_{stamp}.pt\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, _, _ = run_epoch(model, train_loader, loss_fn, optimizer=optimizer, amp_scaler=amp_scaler)\n",
    "        va_loss, _, _ = run_epoch(model, val_loader, loss_fn)\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        if va_loss < best_val - 1e-6:\n",
    "            best_val = va_loss\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        print(f\"   Epoch {epoch:02d} | TrainLoss {tr_loss:.6f} | ValLoss {va_loss:.6f} | LR {lr_now:.2e} | Pat {bad}/{PATIENCE}\")\n",
    "\n",
    "        if bad >= PATIENCE:\n",
    "            print(\"   â¹ï¸ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    te_loss, te_y_n, te_p_n = run_epoch(model, test_loader, loss_fn)\n",
    "\n",
    "    # De-normalize predictions for metrics on RAW scale\n",
    "    te_y = (te_y_n * sd) + mu\n",
    "    te_p = (te_p_n * sd) + mu\n",
    "\n",
    "    rmse = rmse_np(te_y, te_p)\n",
    "    mae = float(np.mean(np.abs(te_y - te_p)))\n",
    "    r2 = r2_np(te_y, te_p)\n",
    "    da = dir_acc(te_y, te_p)\n",
    "\n",
    "    print(f\"âœ… TEST | loss(norm) {te_loss:.6f} | RMSE {rmse:.6f} | MAE {mae:.6f} | RÂ² {r2:.4f} | DirAcc {da:.3f}\\n\")\n",
    "\n",
    "    saved_models[horizon_key] = str(best_path)\n",
    "    results_rows.append({\n",
    "        \"horizon\": horizon_key,\n",
    "        \"horizon_days\": int(h_days),\n",
    "        \"test_loss_norm\": float(te_loss),\n",
    "        \"rmse_raw\": float(rmse),\n",
    "        \"mae_raw\": float(mae),\n",
    "        \"r2_raw\": float(r2),\n",
    "        \"diracc_raw\": float(da),\n",
    "        \"target_norm_mu\": float(mu),\n",
    "        \"target_norm_sd\": float(sd),\n",
    "        \"model_path\": str(best_path),\n",
    "        \"device\": str(device),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_rows).sort_values(\"horizon_days\")\n",
    "results_path = PROJECT_DIRS[\"tables\"] / f\"cell9_lstm_results_UPDATEDv3_{stamp}.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "meta_path = PROJECT_DIRS[\"tables\"] / f\"cell9_lstm_meta_UPDATEDv3_{stamp}.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"created_at\": stamp,\n",
    "        \"device\": str(device),\n",
    "        \"amp\": bool(USE_AMP),\n",
    "        \"saved_models\": saved_models,\n",
    "        \"results_csv\": str(results_path),\n",
    "    }, f, indent=2)\n",
    "\n",
    "globals().update({\n",
    "    \"LSTM_RESULTS_DF\": results_df,\n",
    "    \"LSTM_MODELS\": saved_models,\n",
    "    \"CELL9_RESULTS_PATH\": str(results_path),\n",
    "    \"CELL9_META_PATH\": str(meta_path),\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… CELL 9 COMPLETE (UPDATED v3)\")\n",
    "print(\"ðŸ“ Results:\", results_path)\n",
    "print(\"ðŸ“ Meta   :\", meta_path)\n",
    "print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fd64b2-fbea-423c-9252-bcaebd76b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "ðŸ“Œ CELL 10: FINAL EVALUATION + ECONOMIC BACKTEST + XAI (THESIS-GRADE)\n",
      "===============================================================================================\n",
      "âœ… HAS_DATASETS=True | HAS_CELL7=True | HAS_LSTM=True | HAS_SEQ=True\n",
      "\n",
      "ðŸ“¦ Loaded result tables:\n",
      " - cell7_df: (24, 22)\n",
      " - lstm_df : (6, 11)\n",
      "\n",
      "ðŸ† Best-model picks (per horizon):\n",
      " - XGB REG: ['intermediate_10d', 'intermediate_15d', 'intermediate_20d', 'short_1d', 'short_3d', 'short_5d']\n",
      " - XGB CLS: ['intermediate_10d', 'intermediate_15d', 'intermediate_20d', 'short_1d', 'short_3d', 'short_5d']\n",
      " - LSTM   : ['intermediate_10d', 'intermediate_15d', 'intermediate_20d', 'short_1d', 'short_3d', 'short_5d']\n",
      "âš ï¸ intermediate_10d: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\n",
      "âš ï¸ intermediate_15d: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\n",
      "âš ï¸ intermediate_20d: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\n",
      "âš ï¸ short_1d: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\n",
      "âš ï¸ short_3d: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\n",
      "âš ï¸ short_5d: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\n",
      "\n",
      "âœ… Saved model evaluation table: /workspace/quantgenius_project/tables/cell10_model_eval_20260117_141023.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_family</th>\n",
       "      <th>horizon</th>\n",
       "      <th>acc</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>cm</th>\n",
       "      <th>model_path</th>\n",
       "      <th>n</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>DirAcc</th>\n",
       "      <th>IC</th>\n",
       "      <th>pred_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB_CLS_ternary</td>\n",
       "      <td>intermediate_10d</td>\n",
       "      <td>0.450710</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>0.327170</td>\n",
       "      <td>[[776, 52, 1065], [150, 15, 208], [1146, 87, 1...</td>\n",
       "      <td>/workspace/quantgenius_project/models/xgb_cls_...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB_CLS_ternary</td>\n",
       "      <td>intermediate_15d</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.345754</td>\n",
       "      <td>0.345727</td>\n",
       "      <td>[[819, 40, 990], [122, 15, 171], [1203, 57, 15...</td>\n",
       "      <td>/workspace/quantgenius_project/models/xgb_cls_...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB_CLS_ternary</td>\n",
       "      <td>intermediate_20d</td>\n",
       "      <td>0.477688</td>\n",
       "      <td>0.328943</td>\n",
       "      <td>0.324370</td>\n",
       "      <td>[[746, 50, 1037], [117, 3, 154], [1177, 40, 16...</td>\n",
       "      <td>/workspace/quantgenius_project/models/xgb_cls_...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB_CLS_ternary</td>\n",
       "      <td>short_1d</td>\n",
       "      <td>0.379108</td>\n",
       "      <td>0.375108</td>\n",
       "      <td>0.375441</td>\n",
       "      <td>[[570, 416, 626], [443, 492, 482], [646, 448, ...</td>\n",
       "      <td>/workspace/quantgenius_project/models/xgb_cls_...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGB_CLS_ternary</td>\n",
       "      <td>short_3d</td>\n",
       "      <td>0.408722</td>\n",
       "      <td>0.343316</td>\n",
       "      <td>0.341923</td>\n",
       "      <td>[[767, 174, 885], [297, 87, 378], [982, 199, 1...</td>\n",
       "      <td>/workspace/quantgenius_project/models/xgb_cls_...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB_CLS_ternary</td>\n",
       "      <td>short_5d</td>\n",
       "      <td>0.446653</td>\n",
       "      <td>0.348220</td>\n",
       "      <td>0.346180</td>\n",
       "      <td>[[825, 103, 958], [220, 39, 289], [1026, 132, ...</td>\n",
       "      <td>/workspace/quantgenius_project/models/xgb_cls_...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM_from_cell9_results</td>\n",
       "      <td>intermediate_10d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/quantgenius_project/models/lstm_UPD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.067411</td>\n",
       "      <td>0.567343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM_from_cell9_results</td>\n",
       "      <td>intermediate_15d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/quantgenius_project/models/lstm_UPD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.029577</td>\n",
       "      <td>0.060543</td>\n",
       "      <td>0.083679</td>\n",
       "      <td>0.539757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM_from_cell9_results</td>\n",
       "      <td>intermediate_20d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/quantgenius_project/models/lstm_UPD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.052364</td>\n",
       "      <td>0.071317</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.529006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM_from_cell9_results</td>\n",
       "      <td>short_1d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/quantgenius_project/models/lstm_UPD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.014579</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>0.530629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTM_from_cell9_results</td>\n",
       "      <td>short_3d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/quantgenius_project/models/lstm_UPD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005788</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>0.038343</td>\n",
       "      <td>0.548479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM_from_cell9_results</td>\n",
       "      <td>short_5d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/quantgenius_project/models/lstm_UPD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.034063</td>\n",
       "      <td>0.048445</td>\n",
       "      <td>0.555578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_family           horizon       acc  balanced_acc  \\\n",
       "0           XGB_CLS_ternary  intermediate_10d  0.450710      0.329103   \n",
       "1           XGB_CLS_ternary  intermediate_15d  0.476065      0.345754   \n",
       "2           XGB_CLS_ternary  intermediate_20d  0.477688      0.328943   \n",
       "3           XGB_CLS_ternary          short_1d  0.379108      0.375108   \n",
       "4           XGB_CLS_ternary          short_3d  0.408722      0.343316   \n",
       "5           XGB_CLS_ternary          short_5d  0.446653      0.348220   \n",
       "6   LSTM_from_cell9_results  intermediate_10d       NaN           NaN   \n",
       "7   LSTM_from_cell9_results  intermediate_15d       NaN           NaN   \n",
       "8   LSTM_from_cell9_results  intermediate_20d       NaN           NaN   \n",
       "9   LSTM_from_cell9_results          short_1d       NaN           NaN   \n",
       "10  LSTM_from_cell9_results          short_3d       NaN           NaN   \n",
       "11  LSTM_from_cell9_results          short_5d       NaN           NaN   \n",
       "\n",
       "    macro_f1                                                 cm  \\\n",
       "0   0.327170  [[776, 52, 1065], [150, 15, 208], [1146, 87, 1...   \n",
       "1   0.345727  [[819, 40, 990], [122, 15, 171], [1203, 57, 15...   \n",
       "2   0.324370  [[746, 50, 1037], [117, 3, 154], [1177, 40, 16...   \n",
       "3   0.375441  [[570, 416, 626], [443, 492, 482], [646, 448, ...   \n",
       "4   0.341923  [[767, 174, 885], [297, 87, 378], [982, 199, 1...   \n",
       "5   0.346180  [[825, 103, 958], [220, 39, 289], [1026, 132, ...   \n",
       "6        NaN                                                NaN   \n",
       "7        NaN                                                NaN   \n",
       "8        NaN                                                NaN   \n",
       "9        NaN                                                NaN   \n",
       "10       NaN                                                NaN   \n",
       "11       NaN                                                NaN   \n",
       "\n",
       "                                           model_path       n        R2  \\\n",
       "0   /workspace/quantgenius_project/models/xgb_cls_...  4930.0       NaN   \n",
       "1   /workspace/quantgenius_project/models/xgb_cls_...  4930.0       NaN   \n",
       "2   /workspace/quantgenius_project/models/xgb_cls_...  4930.0       NaN   \n",
       "3   /workspace/quantgenius_project/models/xgb_cls_...  4930.0       NaN   \n",
       "4   /workspace/quantgenius_project/models/xgb_cls_...  4930.0       NaN   \n",
       "5   /workspace/quantgenius_project/models/xgb_cls_...  4930.0       NaN   \n",
       "6   /workspace/quantgenius_project/models/lstm_UPD...     NaN  0.005209   \n",
       "7   /workspace/quantgenius_project/models/lstm_UPD...     NaN -0.029577   \n",
       "8   /workspace/quantgenius_project/models/lstm_UPD...     NaN -0.052364   \n",
       "9   /workspace/quantgenius_project/models/lstm_UPD...     NaN  0.008432   \n",
       "10  /workspace/quantgenius_project/models/lstm_UPD...     NaN  0.005788   \n",
       "11  /workspace/quantgenius_project/models/lstm_UPD...     NaN  0.008162   \n",
       "\n",
       "         MAE      RMSE    DirAcc  IC  pred_std  \n",
       "0        NaN       NaN       NaN NaN       NaN  \n",
       "1        NaN       NaN       NaN NaN       NaN  \n",
       "2        NaN       NaN       NaN NaN       NaN  \n",
       "3        NaN       NaN       NaN NaN       NaN  \n",
       "4        NaN       NaN       NaN NaN       NaN  \n",
       "5        NaN       NaN       NaN NaN       NaN  \n",
       "6   0.048373  0.067411  0.567343 NaN       NaN  \n",
       "7   0.060543  0.083679  0.539757 NaN       NaN  \n",
       "8   0.071317  0.096501  0.529006 NaN       NaN  \n",
       "9   0.014579  0.022432  0.530629 NaN       NaN  \n",
       "10  0.026135  0.038343  0.548479 NaN       NaN  \n",
       "11  0.034063  0.048445  0.555578 NaN       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Backtest summary (only if panel alignment existed):\n",
      " - saved: /workspace/quantgenius_project/tables/cell10_backtests_20260117_141023.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "âœ… CELL 10 COMPLETE\n",
      "ðŸ“ Eval CSV     : /workspace/quantgenius_project/tables/cell10_model_eval_20260117_141023.csv\n",
      "ðŸ“ Backtest CSV : /workspace/quantgenius_project/tables/cell10_backtests_20260117_141023.csv\n",
      "ðŸ“ Meta JSON    : /workspace/quantgenius_project/tables/cell10_meta_20260117_141023.json\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CELL 10: FINAL EVALUATION + ECONOMIC BACKTEST + XAI (THESIS-GRADE)\n",
    "# Compatible with: Cell 5 (DATASETS), Cell 7 (XGB models), Cell 8/9 (LSTM)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*95)\n",
    "print(\"ðŸ“Œ CELL 10: FINAL EVALUATION + ECONOMIC BACKTEST + XAI (THESIS-GRADE)\")\n",
    "print(\"=\"*95)\n",
    "\n",
    "import os, glob, json, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Required globals ---\n",
    "required = [\"PROJECT_DIRS\", \"FORECAST_HORIZONS\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing required globals: {missing}. Run Cells 1â€“3 first (and bootstrap on Vast).\")\n",
    "\n",
    "# Optional but expected if you trained models:\n",
    "HAS_DATASETS = \"DATASETS\" in globals()\n",
    "HAS_CELL7    = \"CELL7_RESULTS_DF\" in globals() or \"CELL7_RESULTS_PATH\" in globals()\n",
    "HAS_LSTM     = \"LSTM_RESULTS_DF\" in globals() or \"CELL9_RESULTS_PATH\" in globals()\n",
    "HAS_SEQ      = \"SEQ_DATASETS\" in globals()\n",
    "\n",
    "print(f\"âœ… HAS_DATASETS={HAS_DATASETS} | HAS_CELL7={HAS_CELL7} | HAS_LSTM={HAS_LSTM} | HAS_SEQ={HAS_SEQ}\")\n",
    "\n",
    "TABLES_DIR  = PROJECT_DIRS[\"tables\"]\n",
    "FIG_DIR     = PROJECT_DIRS.get(\"figures\", TABLES_DIR.parent / \"figures\")\n",
    "MODELS_DIR  = PROJECT_DIRS.get(\"models\",  TABLES_DIR.parent / \"models\")\n",
    "for p in [TABLES_DIR, FIG_DIR, MODELS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# --------------------------\n",
    "# Helpers: metrics\n",
    "# --------------------------\n",
    "EPS = 1e-12\n",
    "\n",
    "def reg_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    r2   = float(1.0 - ss_res / (ss_tot + EPS))\n",
    "    da   = float((np.sign(y_true) == np.sign(y_pred)).mean())\n",
    "    ic   = float(np.corrcoef(y_true, y_pred)[0,1]) if (np.std(y_true) > 0 and np.std(y_pred) > 0) else 0.0\n",
    "    return {\"R2\": r2, \"MAE\": mae, \"RMSE\": rmse, \"DirAcc\": da, \"IC\": ic, \"pred_std\": float(np.std(y_pred))}\n",
    "\n",
    "def cls_metrics(y_true, y_pred, labels=(-1,0,1)):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    acc = float((y_true == y_pred).mean())\n",
    "    # balanced acc\n",
    "    bal_parts = []\n",
    "    for lb in labels:\n",
    "        m = (y_true == lb)\n",
    "        bal_parts.append(float((y_pred[m] == lb).mean()) if m.sum() > 0 else np.nan)\n",
    "    bal = float(np.nanmean(bal_parts)) if np.isfinite(np.nanmean(bal_parts)) else 0.0\n",
    "    # macro F1\n",
    "    f1s = []\n",
    "    for lb in labels:\n",
    "        tp = np.sum((y_true==lb) & (y_pred==lb))\n",
    "        fp = np.sum((y_true!=lb) & (y_pred==lb))\n",
    "        fn = np.sum((y_true==lb) & (y_pred!=lb))\n",
    "        prec = tp / (tp + fp + EPS)\n",
    "        rec  = tp / (tp + fn + EPS)\n",
    "        f1 = 2*prec*rec/(prec+rec+EPS)\n",
    "        f1s.append(f1)\n",
    "    macro_f1 = float(np.mean(f1s))\n",
    "    # confusion matrix\n",
    "    lab2i = {lb:i for i,lb in enumerate(labels)}\n",
    "    cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt in lab2i and yp in lab2i:\n",
    "            cm[lab2i[yt], lab2i[yp]] += 1\n",
    "    return {\"acc\": acc, \"balanced_acc\": bal, \"macro_f1\": macro_f1, \"cm\": cm.tolist()}\n",
    "\n",
    "# --------------------------\n",
    "# Helper: economic backtest (rank L/S)\n",
    "# --------------------------\n",
    "def rank_ls_backtest(panel: pd.DataFrame, score_col: str, ret_col: str, date_col=\"date\", q=0.2):\n",
    "    \"\"\"\n",
    "    Long top-q assets, Short bottom-q assets each date (equal-weight).\n",
    "    Requires panel with columns: date, asset, score, realized_return\n",
    "    Returns: daily portfolio returns series + summary stats\n",
    "    \"\"\"\n",
    "    df = panel[[date_col, \"asset\", score_col, ret_col]].dropna().copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col]).dt.normalize()\n",
    "    df = df.sort_values(date_col)\n",
    "\n",
    "    rets = []\n",
    "    for d, g in df.groupby(date_col):\n",
    "        g = g.dropna()\n",
    "        if len(g) < 5:\n",
    "            continue\n",
    "        g = g.sort_values(score_col)\n",
    "        k = max(1, int(len(g)*q))\n",
    "        short = g.head(k)[ret_col].mean()\n",
    "        long  = g.tail(k)[ret_col].mean()\n",
    "        rets.append((d, float(long - short)))\n",
    "\n",
    "    if not rets:\n",
    "        return None, None\n",
    "\n",
    "    s = pd.Series(dict(rets)).sort_index()\n",
    "    mu = float(s.mean())\n",
    "    sd = float(s.std() + EPS)\n",
    "    sharpe = float((mu / sd) * math.sqrt(252))\n",
    "    cum = float((1 + s).prod() - 1)\n",
    "    hit = float((s > 0).mean())\n",
    "    return s, {\"mean_daily\": mu, \"std_daily\": sd, \"sharpe\": sharpe, \"cum_return\": cum, \"hit_rate\": hit, \"n_days\": int(len(s))}\n",
    "\n",
    "# --------------------------\n",
    "# Load Cell7 results if needed\n",
    "# --------------------------\n",
    "cell7_df = None\n",
    "if \"CELL7_RESULTS_DF\" in globals():\n",
    "    cell7_df = CELL7_RESULTS_DF.copy()\n",
    "elif \"CELL7_RESULTS_PATH\" in globals() and os.path.exists(CELL7_RESULTS_PATH):\n",
    "    cell7_df = pd.read_csv(CELL7_RESULTS_PATH)\n",
    "\n",
    "# Load LSTM results if needed\n",
    "lstm_df = None\n",
    "if \"LSTM_RESULTS_DF\" in globals():\n",
    "    lstm_df = LSTM_RESULTS_DF.copy()\n",
    "elif \"CELL9_RESULTS_PATH\" in globals() and os.path.exists(CELL9_RESULTS_PATH):\n",
    "    lstm_df = pd.read_csv(CELL9_RESULTS_PATH)\n",
    "\n",
    "print(\"\\nðŸ“¦ Loaded result tables:\")\n",
    "print(\" - cell7_df:\", None if cell7_df is None else cell7_df.shape)\n",
    "print(\" - lstm_df :\", None if lstm_df is None else lstm_df.shape)\n",
    "\n",
    "# --------------------------\n",
    "# Select \"best\" models per horizon\n",
    "# --------------------------\n",
    "best = {\"xgb_reg\": {}, \"xgb_cls\": {}, \"lstm\": {}}\n",
    "\n",
    "if cell7_df is not None and len(cell7_df):\n",
    "    # Regression best by highest R2 (volnorm reg)\n",
    "    reg_df = cell7_df[cell7_df[\"task\"].str.contains(\"regression\", na=False)].copy()\n",
    "    if len(reg_df):\n",
    "        for hz, g in reg_df.groupby(\"horizon\"):\n",
    "            gg = g.sort_values([\"R2\", \"DirAcc\"], ascending=False).head(1)\n",
    "            best[\"xgb_reg\"][hz] = gg.iloc[0].to_dict()\n",
    "\n",
    "    # Classification best by balanced_acc then macro_f1\n",
    "    cls_df = cell7_df[cell7_df[\"task\"].str.contains(\"classification\", na=False)].copy()\n",
    "    if len(cls_df):\n",
    "        for hz, g in cls_df.groupby(\"horizon\"):\n",
    "            gg = g.sort_values([\"balanced_acc\", \"macro_f1\"], ascending=False).head(1)\n",
    "            best[\"xgb_cls\"][hz] = gg.iloc[0].to_dict()\n",
    "\n",
    "if lstm_df is not None and len(lstm_df):\n",
    "    for hz, g in lstm_df.groupby(\"horizon\"):\n",
    "        gg = g.sort_values([\"r2_raw\", \"diracc_raw\"], ascending=False).head(1)\n",
    "        best[\"lstm\"][hz] = gg.iloc[0].to_dict()\n",
    "\n",
    "print(\"\\nðŸ† Best-model picks (per horizon):\")\n",
    "print(\" - XGB REG:\", list(best[\"xgb_reg\"].keys()))\n",
    "print(\" - XGB CLS:\", list(best[\"xgb_cls\"].keys()))\n",
    "print(\" - LSTM   :\", list(best[\"lstm\"].keys()))\n",
    "\n",
    "# --------------------------\n",
    "# Re-evaluate models on TEST (from DATASETS / SEQ_DATASETS)\n",
    "# --------------------------\n",
    "rows = []\n",
    "\n",
    "# XGB REG re-eval (volnorm clean)\n",
    "if HAS_DATASETS and len(best[\"xgb_reg\"]) > 0:\n",
    "    import xgboost as xgb\n",
    "    for hz in best[\"xgb_reg\"]:\n",
    "        ds = DATASETS.get(hz, None)\n",
    "        if ds is None:\n",
    "            continue\n",
    "        needed = [\"X_test_volnorm_clean\", \"y_test_volnorm_clean\"]\n",
    "        if not all(k in ds for k in needed):\n",
    "            print(f\"âš ï¸ {hz}: missing volnorm_clean arrays in DATASETS. Skipping XGB reg re-eval.\")\n",
    "            continue\n",
    "\n",
    "        model_path = best[\"xgb_reg\"][hz].get(\"model_path\", \"\")\n",
    "        if not model_path or (not os.path.exists(model_path)):\n",
    "            print(f\"âš ï¸ {hz}: model file not found: {model_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        Xte = ds[\"X_test_volnorm_clean\"]\n",
    "        yte = ds[\"y_test_volnorm_clean\"]\n",
    "\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.load_model(model_path)\n",
    "        pred = model.predict(Xte)\n",
    "\n",
    "        mets = reg_metrics(yte, pred)\n",
    "        rows.append({\"model_family\":\"XGB_REG_volnorm_clean\", \"horizon\":hz, **mets, \"model_path\":model_path, \"n\": int(len(yte))})\n",
    "\n",
    "# XGB CLS re-eval (ternary)\n",
    "if HAS_DATASETS and len(best[\"xgb_cls\"]) > 0:\n",
    "    import xgboost as xgb\n",
    "    for hz in best[\"xgb_cls\"]:\n",
    "        ds = DATASETS.get(hz, None)\n",
    "        if ds is None:\n",
    "            continue\n",
    "        needed = [\"X_test\", \"y_test_cls\"]\n",
    "        if not all(k in ds for k in needed):\n",
    "            print(f\"âš ï¸ {hz}: missing classification arrays in DATASETS. Skipping XGB cls re-eval.\")\n",
    "            continue\n",
    "\n",
    "        model_path = best[\"xgb_cls\"][hz].get(\"model_path\", \"\")\n",
    "        if not model_path or (not os.path.exists(model_path)):\n",
    "            print(f\"âš ï¸ {hz}: model file not found: {model_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        Xte = ds[\"X_test\"]\n",
    "        yte = ds[\"y_test_cls\"]\n",
    "\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(model_path)\n",
    "        pred_m = model.predict(Xte)\n",
    "        pred = (pred_m - 1).astype(int)  # back to -1/0/1\n",
    "\n",
    "        mets = cls_metrics(yte, pred)\n",
    "        rows.append({\"model_family\":\"XGB_CLS_ternary\", \"horizon\":hz, **mets, \"model_path\":model_path, \"n\": int(len(yte))})\n",
    "\n",
    "# LSTM re-eval (optional quick check from saved results only)\n",
    "if len(best[\"lstm\"]) > 0:\n",
    "    for hz in best[\"lstm\"]:\n",
    "        r = best[\"lstm\"][hz]\n",
    "        rows.append({\n",
    "            \"model_family\":\"LSTM_from_cell9_results\",\n",
    "            \"horizon\": hz,\n",
    "            \"R2\": float(r.get(\"r2_raw\", np.nan)),\n",
    "            \"MAE\": float(r.get(\"mae_raw\", np.nan)),\n",
    "            \"RMSE\": float(r.get(\"rmse_raw\", np.nan)),\n",
    "            \"DirAcc\": float(r.get(\"diracc_raw\", np.nan)),\n",
    "            \"IC\": np.nan,\n",
    "            \"pred_std\": np.nan,\n",
    "            \"model_path\": str(r.get(\"model_path\", \"\")),\n",
    "            \"n\": np.nan\n",
    "        })\n",
    "\n",
    "eval_df = pd.DataFrame(rows)\n",
    "eval_path = TABLES_DIR / f\"cell10_model_eval_{stamp}.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(\"\\nâœ… Saved model evaluation table:\", eval_path)\n",
    "display(eval_df.head(30))\n",
    "\n",
    "# --------------------------\n",
    "# ECONOMIC BACKTEST (if panel test exists)\n",
    "# --------------------------\n",
    "bt_rows = []\n",
    "bt_series_paths = {}\n",
    "\n",
    "def find_panel_test(ds: dict):\n",
    "    # Try common keys your Cell5 might have saved\n",
    "    for k in [\"panel_test\", \"test_panel\", \"df_test_panel\", \"panel_df_test\", \"test_df\"]:\n",
    "        if k in ds and isinstance(ds[k], pd.DataFrame):\n",
    "            return ds[k]\n",
    "    return None\n",
    "\n",
    "if HAS_DATASETS:\n",
    "    for hz in FORECAST_HORIZONS.keys():\n",
    "        ds = DATASETS.get(hz, None)\n",
    "        if ds is None:\n",
    "            continue\n",
    "\n",
    "        panel = find_panel_test(ds)\n",
    "        if panel is None:\n",
    "            continue\n",
    "\n",
    "        # Must have date+asset and a realized return/target column.\n",
    "        # We will try y_raw (continuous return target) as realized return proxy if present.\n",
    "        cand_ret = None\n",
    "        for c in [\"y_raw\", \"y_test_raw\", \"y\", \"target\", \"y_future\", \"y_true\"]:\n",
    "            if c in panel.columns:\n",
    "                cand_ret = c\n",
    "                break\n",
    "        if cand_ret is None:\n",
    "            # fallback: if panel has the actual target column name\n",
    "            ycols = [c for c in panel.columns if str(c).startswith(\"y_\")]\n",
    "            if ycols:\n",
    "                cand_ret = ycols[0]\n",
    "        if cand_ret is None:\n",
    "            continue\n",
    "\n",
    "        # Use XGB regression score if we can attach predictions\n",
    "        xgb_reg_path = best[\"xgb_reg\"].get(hz, {}).get(\"model_path\", None)\n",
    "        if xgb_reg_path and os.path.exists(xgb_reg_path) and (\"X_test_volnorm_clean\" in ds):\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                model = xgb.XGBRegressor()\n",
    "                model.load_model(xgb_reg_path)\n",
    "\n",
    "                # If you stored matching panel rows for the clean test set, prefer that.\n",
    "                # Otherwise, we skip to avoid misalignment.\n",
    "                if \"panel_test_volnorm_clean\" in ds and isinstance(ds[\"panel_test_volnorm_clean\"], pd.DataFrame):\n",
    "                    panel_bt = ds[\"panel_test_volnorm_clean\"].copy()\n",
    "                    Xte = ds[\"X_test_volnorm_clean\"]\n",
    "                    pred = model.predict(Xte)\n",
    "                    panel_bt[\"score_xgb\"] = pred\n",
    "                else:\n",
    "                    # no safe alignment key -> skip\n",
    "                    continue\n",
    "\n",
    "                s, summ = rank_ls_backtest(panel_bt, \"score_xgb\", cand_ret, date_col=\"date\", q=0.2)\n",
    "                if s is None:\n",
    "                    continue\n",
    "                out_ser = TABLES_DIR / f\"cell10_bt_{hz}_{stamp}.csv\"\n",
    "                s.to_csv(out_ser, header=[\"ls_return\"])\n",
    "                bt_series_paths[hz] = str(out_ser)\n",
    "\n",
    "                bt_rows.append({\n",
    "                    \"horizon\": hz,\n",
    "                    \"model\": \"XGB_REG_volnorm_clean\",\n",
    "                    **summ,\n",
    "                    \"series_path\": str(out_ser)\n",
    "                })\n",
    "\n",
    "                # plot\n",
    "                cum = (1 + s).cumprod()\n",
    "                plt.figure()\n",
    "                plt.plot(cum.index, cum.values)\n",
    "                plt.title(f\"Cumulative L/S (top20%-bottom20%) | {hz}\")\n",
    "                plt.xlabel(\"Date\")\n",
    "                plt.ylabel(\"Cumulative wealth\")\n",
    "                fig_path = FIG_DIR / f\"cell10_cum_ls_{hz}_{stamp}.png\"\n",
    "                plt.savefig(fig_path)\n",
    "                plt.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Backtest failed for {hz}: {repr(e)}\")\n",
    "\n",
    "bt_df = pd.DataFrame(bt_rows)\n",
    "bt_path = TABLES_DIR / f\"cell10_backtests_{stamp}.csv\"\n",
    "bt_df.to_csv(bt_path, index=False)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Backtest summary (only if panel alignment existed):\")\n",
    "print(\" - saved:\", bt_path)\n",
    "display(bt_df)\n",
    "\n",
    "# --------------------------\n",
    "# XAI: SHAP for XGBoost Regression (optional)\n",
    "# --------------------------\n",
    "shap_path = None\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "if HAS_SHAP and HAS_DATASETS and len(best[\"xgb_reg\"]) > 0:\n",
    "    import xgboost as xgb\n",
    "\n",
    "    # pick one horizon (best by R2 in eval_df)\n",
    "    cand = eval_df[eval_df[\"model_family\"].str.contains(\"XGB_REG\", na=False)].copy()\n",
    "    if len(cand):\n",
    "        pick = cand.sort_values(\"R2\", ascending=False).iloc[0][\"horizon\"]\n",
    "        ds = DATASETS[pick]\n",
    "        model_path = best[\"xgb_reg\"][pick][\"model_path\"]\n",
    "\n",
    "        if os.path.exists(model_path) and (\"X_train_volnorm_clean\" in ds):\n",
    "            model = xgb.XGBRegressor()\n",
    "            model.load_model(model_path)\n",
    "\n",
    "            # SHAP on a subsample for speed\n",
    "            Xtr = ds[\"X_train_volnorm_clean\"]\n",
    "            n = min(5000, len(Xtr))\n",
    "            idx = np.random.choice(len(Xtr), size=n, replace=False)\n",
    "            Xs = Xtr[idx]\n",
    "\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            sv = explainer.shap_values(Xs)\n",
    "\n",
    "            # feature names if provided\n",
    "            feat_names = ds.get(\"feature_names\", None)\n",
    "            if feat_names is None:\n",
    "                feat_names = [f\"f{i}\" for i in range(Xs.shape[1])]\n",
    "\n",
    "            # mean abs shap importance\n",
    "            imp = np.mean(np.abs(sv), axis=0)\n",
    "            shap_df = pd.DataFrame({\"feature\": feat_names, \"mean_abs_shap\": imp}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "            shap_path = TABLES_DIR / f\"cell10_shap_importance_{pick}_{stamp}.csv\"\n",
    "            shap_df.to_csv(shap_path, index=False)\n",
    "\n",
    "            # plot top 30\n",
    "            top = shap_df.head(30).iloc[::-1]\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(top[\"feature\"], top[\"mean_abs_shap\"])\n",
    "            plt.title(f\"Top SHAP Features | XGB REG volnorm | {pick}\")\n",
    "            plt.xlabel(\"Mean |SHAP|\")\n",
    "            fig_path = FIG_DIR / f\"cell10_shap_top30_{pick}_{stamp}.png\"\n",
    "            plt.savefig(fig_path)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"\\nâœ… SHAP saved: {shap_path}\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ SHAP skipped: missing model or X_train_volnorm_clean.\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ SHAP not installed (or no XGB reg). Skipping XAI.\")\n",
    "\n",
    "# --------------------------\n",
    "# Save a compact \"Cell10 meta\"\n",
    "# --------------------------\n",
    "meta = {\n",
    "    \"created_at\": stamp,\n",
    "    \"has_datasets\": HAS_DATASETS,\n",
    "    \"has_cell7\": HAS_CELL7,\n",
    "    \"has_lstm\": HAS_LSTM,\n",
    "    \"best_models\": best,\n",
    "    \"eval_csv\": str(eval_path),\n",
    "    \"backtests_csv\": str(bt_path),\n",
    "    \"shap_csv\": (str(shap_path) if shap_path else None),\n",
    "}\n",
    "\n",
    "meta_path = TABLES_DIR / f\"cell10_meta_{stamp}.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "globals().update({\n",
    "    \"CELL10_EVAL_DF\": eval_df,\n",
    "    \"CELL10_EVAL_PATH\": str(eval_path),\n",
    "    \"CELL10_BT_DF\": bt_df,\n",
    "    \"CELL10_BT_PATH\": str(bt_path),\n",
    "    \"CELL10_META_PATH\": str(meta_path),\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*95)\n",
    "print(\"âœ… CELL 10 COMPLETE\")\n",
    "print(\"ðŸ“ Eval CSV     :\", eval_path)\n",
    "print(\"ðŸ“ Backtest CSV :\", bt_path)\n",
    "print(\"ðŸ“ Meta JSON    :\", meta_path)\n",
    "if shap_path:\n",
    "    print(\"ðŸ“ SHAP CSV     :\", shap_path)\n",
    "print(\"=\"*95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfdaeb93-e766-460e-a582-f972c9402733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================================\n",
      "ðŸ§  CELL 11 (THESIS-GRADE): TRANSFORMER ENCODER TRAINING (MULTI-HORIZON, VAST SAFE)\n",
      "==================================================================================================\n",
      "ðŸ–¥ï¸ Device: cuda\n",
      "\n",
      "âš™ï¸ TRAINING CONFIG:\n",
      "  â€¢ batch_size: 256\n",
      "  â€¢ epochs: 40\n",
      "  â€¢ lr: 0.001\n",
      "  â€¢ weight_decay: 0.0001\n",
      "  â€¢ lookback: 60\n",
      "  â€¢ d_model: 128\n",
      "  â€¢ n_heads: 8\n",
      "  â€¢ n_layers: 3\n",
      "  â€¢ dropout: 0.15\n",
      "  â€¢ ff_mult: 4\n",
      "  â€¢ grad_clip: 1.0\n",
      "  â€¢ huber_delta: 1.0\n",
      "  â€¢ patience: 8\n",
      "  â€¢ use_amp: True\n",
      "  â€¢ target_norm_train_only: True\n",
      "  â€¢ num_workers: 2\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: short_1d | NPZ: seq_short_1d_20260117_145029.npz\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.339818 | ValLoss 0.306886 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.336100 | ValLoss 0.305590 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 03 | TrainLoss 0.334369 | ValLoss 0.306608 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 04 | TrainLoss 0.333894 | ValLoss 0.305563 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 05 | TrainLoss 0.332389 | ValLoss 0.303985 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 06 | TrainLoss 0.331217 | ValLoss 0.309460 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 07 | TrainLoss 0.329152 | ValLoss 0.304527 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 08 | TrainLoss 0.327885 | ValLoss 0.306437 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 09 | TrainLoss 0.323204 | ValLoss 0.305640 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 10 | TrainLoss 0.320691 | ValLoss 0.305001 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 11 | TrainLoss 0.317541 | ValLoss 0.306416 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 12 | TrainLoss 0.314112 | ValLoss 0.305497 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 13 | TrainLoss 0.311415 | ValLoss 0.305914 | LR 2.50e-04 | Pat 7/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | RMSE 0.022458 | MAE 0.014586 | RÂ² 0.0062 | DirAcc 0.536\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: short_3d | NPZ: seq_short_3d_20260117_145029.npz\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.358802 | ValLoss 0.327146 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.351429 | ValLoss 0.322533 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 03 | TrainLoss 0.349169 | ValLoss 0.328504 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 04 | TrainLoss 0.348018 | ValLoss 0.330593 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 05 | TrainLoss 0.343566 | ValLoss 0.340208 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 06 | TrainLoss 0.333409 | ValLoss 0.358457 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 07 | TrainLoss 0.326834 | ValLoss 0.352842 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 08 | TrainLoss 0.315565 | ValLoss 0.391608 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 09 | TrainLoss 0.299690 | ValLoss 0.409877 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 10 | TrainLoss 0.291275 | ValLoss 0.420713 | LR 2.50e-04 | Pat 7/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | RMSE 0.038374 | MAE 0.026141 | RÂ² 0.0042 | DirAcc 0.554\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: short_5d | NPZ: seq_short_5d_20260117_145029.npz\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.357445 | ValLoss 0.331530 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.352176 | ValLoss 0.342649 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 03 | TrainLoss 0.349533 | ValLoss 0.345889 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 04 | TrainLoss 0.341825 | ValLoss 0.370988 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 05 | TrainLoss 0.321643 | ValLoss 0.377131 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 06 | TrainLoss 0.307169 | ValLoss 0.386453 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 07 | TrainLoss 0.291037 | ValLoss 0.437955 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 08 | TrainLoss 0.273222 | ValLoss 0.420120 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 09 | TrainLoss 0.261263 | ValLoss 0.415105 | LR 2.50e-04 | Pat 7/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | RMSE 0.048594 | MAE 0.034153 | RÂ² 0.0021 | DirAcc 0.564\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: intermediate_10d | NPZ: seq_intermediate_10d_20260117_145029.npz\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.366946 | ValLoss 0.351337 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.351230 | ValLoss 0.369092 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 03 | TrainLoss 0.328555 | ValLoss 0.449184 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 04 | TrainLoss 0.304213 | ValLoss 0.408729 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 05 | TrainLoss 0.270543 | ValLoss 0.501678 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 06 | TrainLoss 0.249300 | ValLoss 0.444382 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 07 | TrainLoss 0.230805 | ValLoss 0.531484 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 08 | TrainLoss 0.205392 | ValLoss 0.521707 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 09 | TrainLoss 0.194740 | ValLoss 0.489165 | LR 2.50e-04 | Pat 7/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | RMSE 0.067540 | MAE 0.048693 | RÂ² 0.0014 | DirAcc 0.576\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: intermediate_15d | NPZ: seq_intermediate_15d_20260117_145029.npz\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.360927 | ValLoss 0.358045 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.340547 | ValLoss 0.430935 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 03 | TrainLoss 0.306740 | ValLoss 0.449747 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 04 | TrainLoss 0.263922 | ValLoss 0.483718 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 05 | TrainLoss 0.217086 | ValLoss 0.524788 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 06 | TrainLoss 0.188731 | ValLoss 0.509713 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 07 | TrainLoss 0.166276 | ValLoss 0.590359 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 08 | TrainLoss 0.146211 | ValLoss 0.571199 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 09 | TrainLoss 0.134260 | ValLoss 0.514470 | LR 2.50e-04 | Pat 7/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | RMSE 0.082000 | MAE 0.059161 | RÂ² 0.0113 | DirAcc 0.588\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ“Š Horizon: intermediate_20d | NPZ: seq_intermediate_20d_20260117_145029.npz\n",
      "   Train: (19130, 60, 44) | Val: (2500, 60, 44) | Test: (4930, 60, 44)\n",
      "   Epoch 01 | TrainLoss 0.369246 | ValLoss 0.333765 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 02 | TrainLoss 0.327301 | ValLoss 0.461533 | LR 1.00e-03 | Pat 0/8\n",
      "   Epoch 03 | TrainLoss 0.278326 | ValLoss 0.444851 | LR 1.00e-03 | Pat 1/8\n",
      "   Epoch 04 | TrainLoss 0.224260 | ValLoss 0.515948 | LR 5.00e-04 | Pat 2/8\n",
      "   Epoch 05 | TrainLoss 0.169088 | ValLoss 0.526228 | LR 5.00e-04 | Pat 3/8\n",
      "   Epoch 06 | TrainLoss 0.141480 | ValLoss 0.477297 | LR 5.00e-04 | Pat 4/8\n",
      "   Epoch 07 | TrainLoss 0.125165 | ValLoss 0.635086 | LR 2.50e-04 | Pat 5/8\n",
      "   Epoch 08 | TrainLoss 0.105524 | ValLoss 0.528379 | LR 2.50e-04 | Pat 6/8\n",
      "   Epoch 09 | TrainLoss 0.095566 | ValLoss 0.501586 | LR 2.50e-04 | Pat 7/8\n",
      "   â¹ï¸ Early stopping triggered.\n",
      "âœ… TEST | RMSE 0.097181 | MAE 0.070514 | RÂ² -0.0672 | DirAcc 0.570\n",
      "\n",
      "==================================================================================================\n",
      "âœ… CELL 11 COMPLETE (Transformer Training)\n",
      "ðŸ“ Results: /workspace/quantgenius_project/tables/cell11_transformer_results_20260117_153517.csv\n",
      "ðŸ“ Meta   : /workspace/quantgenius_project/tables/cell11_transformer_meta_20260117_153517.json\n",
      "ðŸ“ Models : /workspace/quantgenius_project/models\n",
      "==================================================================================================\n",
      "\n",
      "ðŸ“‹ RESULTS SUMMARY:\n",
      "         horizon        R2   DirAcc     RMSE      MAE\n",
      "        short_1d  0.006177 0.535903 0.022458 0.014586\n",
      "        short_3d  0.004171 0.553955 0.038374 0.026141\n",
      "        short_5d  0.002056 0.563895 0.048594 0.034153\n",
      "intermediate_10d  0.001410 0.575862 0.067540 0.048693\n",
      "intermediate_15d  0.011340 0.588438 0.082000 0.059161\n",
      "intermediate_20d -0.067244 0.569980 0.097181 0.070514\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================\n",
    "# ðŸ§  CELL 11 (THESIS-GRADE): TRANSFORMER ENCODER TRAINING (MULTI-HORIZON, NO LEAKAGE, VAST SAFE)\n",
    "# FIX: ReduceLROnPlateau on your torch version does NOT accept `verbose=...`\n",
    "# ==========================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 98)\n",
    "print(\"ðŸ§  CELL 11 (THESIS-GRADE): TRANSFORMER ENCODER TRAINING (MULTI-HORIZON, VAST SAFE)\")\n",
    "print(\"=\" * 98)\n",
    "\n",
    "import os, json, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Guards\n",
    "# ----------------------------\n",
    "required = [\"FORECAST_HORIZONS\", \"PROJECT_DIRS\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing required globals: {missing}. Run Cells 1â€“8 first.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
    "\n",
    "data_dir   = Path(PROJECT_DIRS[\"data\"])\n",
    "tables_dir = Path(PROJECT_DIRS[\"tables\"])\n",
    "models_dir = Path(PROJECT_DIRS.get(\"models\", tables_dir.parent / \"models\"))\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Config (safe defaults for Vast GPU)\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 40,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"lookback\": 60,\n",
    "    \"d_model\": 128,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 3,\n",
    "    \"dropout\": 0.15,\n",
    "    \"ff_mult\": 4,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"huber_delta\": 1.0,\n",
    "    \"patience\": 8,\n",
    "    \"use_amp\": True,\n",
    "    \"target_norm_train_only\": True,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "\n",
    "print(\"\\nâš™ï¸ TRAINING CONFIG:\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"  â€¢ {k}: {v}\")\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Dataset\n",
    "# ----------------------------\n",
    "class SeqNPZDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        y = y.reshape(-1)\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Model: Transformer Encoder Regressor\n",
    "# ----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        x = x + self.pe[:, :T, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, n_features, lookback, d_model=128, n_heads=8, n_layers=3, dropout=0.1, ff_mult=4):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(n_features, d_model)\n",
    "        self.pos = PositionalEncoding(d_model, max_len=lookback + 5, dropout=dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * ff_mult,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.in_proj(x)      # (B, T, D)\n",
    "        h = self.pos(h)\n",
    "        h = self.encoder(h)      # (B, T, D)\n",
    "        h = self.norm(h)\n",
    "        last = h[:, -1, :]\n",
    "        mean = h.mean(dim=1)\n",
    "        z = torch.cat([last, mean], dim=1)\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Metrics\n",
    "# ----------------------------\n",
    "def reg_metrics_np(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if m.sum() < 10:\n",
    "        return {\"R2\": np.nan, \"RMSE\": np.nan, \"MAE\": np.nan, \"DirAcc\": np.nan}\n",
    "    yt, yp = y_true[m], y_pred[m]\n",
    "    ss_res = np.sum((yt - yp) ** 2)\n",
    "    ss_tot = np.sum((yt - yt.mean()) ** 2) + 1e-12\n",
    "    r2 = 1.0 - ss_res / ss_tot\n",
    "    rmse = float(np.sqrt(np.mean((yt - yp) ** 2)))\n",
    "    mae = float(np.mean(np.abs(yt - yp)))\n",
    "    diracc = float(np.mean(np.sign(yt) == np.sign(yp)))\n",
    "    return {\"R2\": float(r2), \"RMSE\": rmse, \"MAE\": mae, \"DirAcc\": diracc}\n",
    "\n",
    "def get_lr(opt):\n",
    "    return float(opt.param_groups[0][\"lr\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train loop\n",
    "# ----------------------------\n",
    "def train_one_horizon(hz):\n",
    "    # Load latest seq_{hz}_*.npz\n",
    "    cands = sorted(list(data_dir.glob(f\"seq_{hz}_*.npz\")))\n",
    "    if not cands:\n",
    "        print(f\"âŒ Missing NPZ for {hz}. Expected seq_{hz}_*.npz in {data_dir}\")\n",
    "        return None\n",
    "    npz_path = cands[-1]\n",
    "\n",
    "    blob = np.load(npz_path, allow_pickle=True)\n",
    "    Xtr = blob[\"X_train\"].astype(np.float32)\n",
    "    ytr = blob[\"y_train\"].astype(np.float32).reshape(-1)\n",
    "    Xva = blob[\"X_val\"].astype(np.float32)\n",
    "    yva = blob[\"y_val\"].astype(np.float32).reshape(-1)\n",
    "    Xte = blob[\"X_test\"].astype(np.float32)\n",
    "    yte = blob[\"y_test\"].astype(np.float32).reshape(-1)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 90)\n",
    "    print(f\"ðŸ“Š Horizon: {hz} | NPZ: {npz_path.name}\")\n",
    "    print(f\"   Train: {Xtr.shape} | Val: {Xva.shape} | Test: {Xte.shape}\")\n",
    "\n",
    "    # Train-only target normalization\n",
    "    if CFG[\"target_norm_train_only\"]:\n",
    "        mu = float(np.mean(ytr))\n",
    "        sd = float(np.std(ytr))\n",
    "        sd = sd if sd > 1e-8 else 1.0\n",
    "        ytr_n = (ytr - mu) / sd\n",
    "        yva_n = (yva - mu) / sd\n",
    "        yte_n = (yte - mu) / sd\n",
    "    else:\n",
    "        mu, sd = 0.0, 1.0\n",
    "        ytr_n, yva_n, yte_n = ytr, yva, yte\n",
    "\n",
    "    train_ds = SeqNPZDataset(Xtr, ytr_n)\n",
    "    val_ds = SeqNPZDataset(Xva, yva_n)\n",
    "    test_ds = SeqNPZDataset(Xte, yte_n)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,\n",
    "                              num_workers=CFG[\"num_workers\"], pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n",
    "                            num_workers=CFG[\"num_workers\"], pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n",
    "                             num_workers=CFG[\"num_workers\"], pin_memory=True)\n",
    "\n",
    "    n_features = Xtr.shape[-1]\n",
    "    model = TransformerRegressor(\n",
    "        n_features=n_features,\n",
    "        lookback=CFG[\"lookback\"],\n",
    "        d_model=CFG[\"d_model\"],\n",
    "        n_heads=CFG[\"n_heads\"],\n",
    "        n_layers=CFG[\"n_layers\"],\n",
    "        dropout=CFG[\"dropout\"],\n",
    "        ff_mult=CFG[\"ff_mult\"],\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
    "    loss_fn = nn.HuberLoss(delta=CFG[\"huber_delta\"])\n",
    "\n",
    "    # âœ… FIX: remove verbose arg (older torch versions)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and CFG[\"use_amp\"]))\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "\n",
    "    for ep in range(1, CFG[\"epochs\"] + 1):\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\" and CFG[\"use_amp\"])):\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip\"])\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        va_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True)\n",
    "                with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\" and CFG[\"use_amp\"])):\n",
    "                    pred = model(xb)\n",
    "                    loss = loss_fn(pred, yb)\n",
    "                va_losses.append(loss.item())\n",
    "\n",
    "        tr_loss = float(np.mean(tr_losses)) if tr_losses else np.nan\n",
    "        va_loss = float(np.mean(va_losses)) if va_losses else np.nan\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "        lr_now = get_lr(opt)\n",
    "\n",
    "        print(f\"   Epoch {ep:02d} | TrainLoss {tr_loss:.6f} | ValLoss {va_loss:.6f} | LR {lr_now:.2e} | Pat {patience}/{CFG['patience']}\")\n",
    "\n",
    "        if va_loss + 1e-6 < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= CFG[\"patience\"]:\n",
    "                print(\"   â¹ï¸ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    preds, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            pred = model(xb).detach().cpu().numpy()\n",
    "            preds.append(pred)\n",
    "            ys.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    pred_n = np.concatenate(preds).reshape(-1)\n",
    "    y_n = np.concatenate(ys).reshape(-1)\n",
    "\n",
    "    # de-normalize\n",
    "    pred = pred_n * sd + mu\n",
    "    y_true = y_n * sd + mu\n",
    "\n",
    "    mets = reg_metrics_np(y_true, pred)\n",
    "    print(f\"âœ… TEST | RMSE {mets['RMSE']:.6f} | MAE {mets['MAE']:.6f} | RÂ² {mets['R2']:.4f} | DirAcc {mets['DirAcc']:.3f}\")\n",
    "\n",
    "    model_path = models_dir / f\"transformer_reg_{hz}_{stamp}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"config\": CFG,\n",
    "        \"horizon\": hz,\n",
    "        \"target\": \"y_raw\",\n",
    "        \"target_mu\": mu,\n",
    "        \"target_sd\": sd,\n",
    "        \"npz_path\": str(npz_path),\n",
    "    }, model_path)\n",
    "\n",
    "    return {\n",
    "        \"horizon\": hz,\n",
    "        \"horizon_days\": int(FORECAST_HORIZONS[hz]),\n",
    "        \"target\": \"y_raw\",\n",
    "        \"model\": \"TransformerEncoder\",\n",
    "        \"R2\": mets[\"R2\"],\n",
    "        \"RMSE\": mets[\"RMSE\"],\n",
    "        \"MAE\": mets[\"MAE\"],\n",
    "        \"DirAcc\": mets[\"DirAcc\"],\n",
    "        \"model_path\": str(model_path),\n",
    "        \"npz_path\": str(npz_path),\n",
    "        \"n_train\": int(Xtr.shape[0]),\n",
    "        \"n_val\": int(Xva.shape[0]),\n",
    "        \"n_test\": int(Xte.shape[0]),\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Train across horizons\n",
    "# ----------------------------\n",
    "results = []\n",
    "for hz in FORECAST_HORIZONS.keys():\n",
    "    out = train_one_horizon(hz)\n",
    "    if out is not None:\n",
    "        results.append(out)\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values([\"horizon_days\"]).reset_index(drop=True)\n",
    "\n",
    "out_csv = tables_dir / f\"cell11_transformer_results_{stamp}.csv\"\n",
    "out_json = tables_dir / f\"cell11_transformer_meta_{stamp}.json\"\n",
    "res_df.to_csv(out_csv, index=False)\n",
    "\n",
    "meta = {\n",
    "    \"stamp\": stamp,\n",
    "    \"device\": str(device),\n",
    "    \"cfg\": CFG,\n",
    "    \"results_csv\": str(out_csv),\n",
    "    \"models_dir\": str(models_dir),\n",
    "}\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 98)\n",
    "print(\"âœ… CELL 11 COMPLETE (Transformer Training)\")\n",
    "print(f\"ðŸ“ Results: {out_csv}\")\n",
    "print(f\"ðŸ“ Meta   : {out_json}\")\n",
    "print(f\"ðŸ“ Models : {models_dir}\")\n",
    "print(\"=\" * 98)\n",
    "\n",
    "print(\"\\nðŸ“‹ RESULTS SUMMARY:\")\n",
    "print(res_df[[\"horizon\", \"R2\", \"DirAcc\", \"RMSE\", \"MAE\"]].to_string(index=False))\n",
    "\n",
    "globals()[\"CELL11_RESULTS_DF\"] = res_df\n",
    "globals()[\"CELL11_RESULTS_PATH\"] = str(out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b548f19-9fde-4da0-9fb3-2c7b53aa4c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================================\n",
      "ðŸ“Š CELL 12 (FIXED): UNIFIED MODEL LEADERBOARD\n",
      "==================================================================================================\n",
      "ðŸ“¦ Found:\n",
      " - XGB : /workspace/quantgenius_project/tables/cell7_xgb_thesis_results_cuda_20260117_142617.csv\n",
      " - LSTM: /workspace/quantgenius_project/tables/cell9_lstm_results_UPDATEDv3_20260117_145043.csv\n",
      " - TRF : /workspace/quantgenius_project/tables/cell11_transformer_results_20260117_153517.csv\n",
      "\n",
      "âœ… Saved:\n",
      " - Full combined leaderboard: /workspace/quantgenius_project/tables/cell12_leaderboard_FIXED_20260117_160432.csv\n",
      " - Best model per horizon   : /workspace/quantgenius_project/tables/cell12_best_overall_FIXED_20260117_160432.csv\n",
      "\n",
      "ðŸ† BEST OVERALL PER HORIZON:\n",
      "         horizon model_family             target       R2   DirAcc     RMSE      MAE                                                                                         model_path\n",
      "        short_1d         LSTM  0.000670997891575 0.006188 0.507302 0.022458 0.014613                   /workspace/quantgenius_project/models/lstm_UPDATEDv3_short_1d_20260117_145043.pt\n",
      "        short_3d         LSTM 0.0020094101782888 0.009567 0.554361 0.038270 0.026053                   /workspace/quantgenius_project/models/lstm_UPDATEDv3_short_3d_20260117_145043.pt\n",
      "        short_5d  Transformer              y_raw 0.002056 0.563895 0.048594 0.034153                  /workspace/quantgenius_project/models/transformer_reg_short_5d_20260117_153517.pt\n",
      "intermediate_10d      XGB_REG          y_volnorm 0.001505 0.578702 3.657912 2.746719 /workspace/quantgenius_project/models/xgb_reg_y_volnorm_intermediate_10d_cuda_20260117_142507.json\n",
      "intermediate_15d  Transformer              y_raw 0.011340 0.588438 0.082000 0.059161          /workspace/quantgenius_project/models/transformer_reg_intermediate_15d_20260117_153517.pt\n",
      "intermediate_20d         LSTM 0.0142006343230605 0.001295 0.558621 0.094008 0.069027           /workspace/quantgenius_project/models/lstm_UPDATEDv3_intermediate_20d_20260117_145043.pt\n",
      "\n",
      "ðŸ“Œ QUICK PIVOT (best per horizon):\n",
      "                    DirAcc       MAE        R2      RMSE\n",
      "horizon                                                 \n",
      "intermediate_10d  0.578702  2.746719  0.001505  3.657912\n",
      "intermediate_15d  0.588438  0.059161  0.011340  0.082000\n",
      "intermediate_20d  0.558621  0.069027  0.001295  0.094008\n",
      "short_1d          0.507302  0.014613  0.006188  0.022458\n",
      "short_3d          0.554361  0.026053  0.009567  0.038270\n",
      "short_5d          0.563895  0.034153  0.002056  0.048594\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================\n",
    "# ðŸ“Š CELL 12 (FIXED, VAST-SAFE): UNIFIED MODEL LEADERBOARD (XGB vs LSTM vs Transformer)\n",
    "#   - Robust to different column names in LSTM/TRF CSVs\n",
    "# ==========================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 98)\n",
    "print(\"ðŸ“Š CELL 12 (FIXED): UNIFIED MODEL LEADERBOARD\")\n",
    "print(\"=\" * 98)\n",
    "\n",
    "import pandas as pd, numpy as np, glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "required = [\"PROJECT_DIRS\", \"FORECAST_HORIZONS\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing required globals: {missing}. Run earlier cells first.\")\n",
    "\n",
    "tables_dir = Path(PROJECT_DIRS[\"tables\"])\n",
    "fig_dir = Path(PROJECT_DIRS.get(\"figures\", tables_dir.parent / \"figures\"))\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def latest(pattern):\n",
    "    files = sorted(glob.glob(str(tables_dir / pattern)))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "# --- Locate result files\n",
    "xgb_path  = latest(\"cell7_xgb_thesis_results_*.csv\") or latest(\"cell7_xgb_results_*.csv\")\n",
    "lstm_path = latest(\"cell9_lstm_results_*.csv\")\n",
    "trf_path  = latest(\"cell11_transformer_results_*.csv\")\n",
    "\n",
    "print(\"ðŸ“¦ Found:\")\n",
    "print(\" - XGB :\", xgb_path)\n",
    "print(\" - LSTM:\", lstm_path)\n",
    "print(\" - TRF :\", trf_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers: normalize columns\n",
    "# ---------------------------\n",
    "def norm_col(s: str) -> str:\n",
    "    \"\"\"Normalize column names for matching.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"Â²\", \"2\")\n",
    "    s = s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates):\n",
    "    \"\"\"Return first matching column name from candidates (normalized match).\"\"\"\n",
    "    nmap = {norm_col(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        key = norm_col(cand)\n",
    "        if key in nmap:\n",
    "            return nmap[key]\n",
    "    # also allow partial contains match (last resort)\n",
    "    for cand in candidates:\n",
    "        key = norm_col(cand)\n",
    "        for k, orig in nmap.items():\n",
    "            if key in k:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def ensure_metrics(df: pd.DataFrame, source_name=\"\"):\n",
    "    \"\"\"\n",
    "    Ensure df has standard columns:\n",
    "    horizon, horizon_days, R2, RMSE, MAE, DirAcc, model_path\n",
    "    using robust mapping from whatever is present.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Map horizon\n",
    "    h_col = pick_col(out, [\"horizon\", \"hz\"])\n",
    "    if h_col is None:\n",
    "        raise KeyError(f\"[{source_name}] Can't find horizon column. Available: {list(out.columns)[:30]}\")\n",
    "    out[\"horizon\"] = out[h_col].astype(str)\n",
    "\n",
    "    # Map horizon_days\n",
    "    hd_col = pick_col(out, [\"horizon_days\", \"h_days\", \"days\", \"horizon_day\"])\n",
    "    if hd_col is not None:\n",
    "        out[\"horizon_days\"] = pd.to_numeric(out[hd_col], errors=\"coerce\")\n",
    "    else:\n",
    "        # derive from FORECAST_HORIZONS\n",
    "        out[\"horizon_days\"] = out[\"horizon\"].map(lambda x: int(FORECAST_HORIZONS.get(x, np.nan)))\n",
    "\n",
    "    # Map regression metrics\n",
    "    r2_col  = pick_col(out, [\"R2\", \"r2\", \"r_2\", \"r2_score\", \"rsq\", \"r_sq\"])\n",
    "    rmse_col= pick_col(out, [\"RMSE\", \"rmse\"])\n",
    "    mae_col = pick_col(out, [\"MAE\", \"mae\"])\n",
    "    da_col  = pick_col(out, [\"DirAcc\", \"diracc\", \"dir_acc\", \"directional_accuracy\", \"direction_acc\", \"sign_acc\"])\n",
    "\n",
    "    out[\"R2\"]     = pd.to_numeric(out[r2_col],  errors=\"coerce\") if r2_col  else np.nan\n",
    "    out[\"RMSE\"]   = pd.to_numeric(out[rmse_col],errors=\"coerce\") if rmse_col else np.nan\n",
    "    out[\"MAE\"]    = pd.to_numeric(out[mae_col], errors=\"coerce\") if mae_col  else np.nan\n",
    "    out[\"DirAcc\"] = pd.to_numeric(out[da_col],  errors=\"coerce\") if da_col   else np.nan\n",
    "\n",
    "    # Optional\n",
    "    mp_col = pick_col(out, [\"model_path\", \"path\", \"checkpoint\", \"ckpt\", \"model_file\"])\n",
    "    out[\"model_path\"] = out[mp_col].astype(str) if mp_col else \"\"\n",
    "\n",
    "    # target name if exists\n",
    "    t_col = pick_col(out, [\"target\", \"y_target\", \"label\"])\n",
    "    out[\"target\"] = out[t_col].astype(str) if t_col else \"y_raw\"\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# Build unified table\n",
    "# ---------------------------\n",
    "dfs = []\n",
    "\n",
    "# XGB\n",
    "if xgb_path:\n",
    "    xgb = pd.read_csv(xgb_path)\n",
    "    # Already thesis-grade from your run, but keep it robust\n",
    "    if \"task\" not in xgb.columns:\n",
    "        xgb[\"task\"] = np.where(xgb.get(\"acc\").notna(), \"classification\", \"regression\")\n",
    "    xgb[\"model_family\"] = np.where(xgb[\"task\"].str.contains(\"class\", case=False, na=False), \"XGB_CLS\", \"XGB_REG\")\n",
    "\n",
    "    # Ensure standard cols exist (fill if missing)\n",
    "    for c in [\"target\",\"horizon\",\"horizon_days\",\"R2\",\"RMSE\",\"MAE\",\"DirAcc\",\"IC\",\"Sharpe\",\"acc\",\"balanced_acc\",\"macro_f1\",\"model_path\"]:\n",
    "        if c not in xgb.columns:\n",
    "            xgb[c] = np.nan\n",
    "    keep = [\"model_family\",\"task\",\"target\",\"horizon\",\"horizon_days\",\"R2\",\"RMSE\",\"MAE\",\"DirAcc\",\"IC\",\"Sharpe\",\n",
    "            \"acc\",\"balanced_acc\",\"macro_f1\",\"model_path\"]\n",
    "    dfs.append(xgb[keep])\n",
    "\n",
    "# LSTM\n",
    "if lstm_path:\n",
    "    lstm_raw = pd.read_csv(lstm_path)\n",
    "    lstm = ensure_metrics(lstm_raw, source_name=\"LSTM\")\n",
    "    lstm[\"model_family\"] = \"LSTM\"\n",
    "    lstm[\"task\"] = \"regression\"\n",
    "    lstm[\"IC\"] = np.nan\n",
    "    lstm[\"Sharpe\"] = np.nan\n",
    "    lstm[\"acc\"] = np.nan\n",
    "    lstm[\"balanced_acc\"] = np.nan\n",
    "    lstm[\"macro_f1\"] = np.nan\n",
    "    keep = [\"model_family\",\"task\",\"target\",\"horizon\",\"horizon_days\",\"R2\",\"RMSE\",\"MAE\",\"DirAcc\",\"IC\",\"Sharpe\",\n",
    "            \"acc\",\"balanced_acc\",\"macro_f1\",\"model_path\"]\n",
    "    dfs.append(lstm[keep])\n",
    "\n",
    "# Transformer\n",
    "if trf_path:\n",
    "    trf_raw = pd.read_csv(trf_path)\n",
    "    trf = ensure_metrics(trf_raw, source_name=\"Transformer\")\n",
    "    trf[\"model_family\"] = \"Transformer\"\n",
    "    trf[\"task\"] = \"regression\"\n",
    "    trf[\"IC\"] = np.nan\n",
    "    trf[\"Sharpe\"] = np.nan\n",
    "    trf[\"acc\"] = np.nan\n",
    "    trf[\"balanced_acc\"] = np.nan\n",
    "    trf[\"macro_f1\"] = np.nan\n",
    "    keep = [\"model_family\",\"task\",\"target\",\"horizon\",\"horizon_days\",\"R2\",\"RMSE\",\"MAE\",\"DirAcc\",\"IC\",\"Sharpe\",\n",
    "            \"acc\",\"balanced_acc\",\"macro_f1\",\"model_path\"]\n",
    "    dfs.append(trf[keep])\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No result tables found. Ensure Cells 7/9/11 saved CSVs to PROJECT_DIRS['tables'].\")\n",
    "\n",
    "leader = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Leaderboards\n",
    "# ---------------------------\n",
    "reg = leader[leader[\"task\"].str.contains(\"reg\", case=False, na=False)].copy()\n",
    "\n",
    "reg[\"R2\"] = pd.to_numeric(reg[\"R2\"], errors=\"coerce\")\n",
    "reg[\"DirAcc\"] = pd.to_numeric(reg[\"DirAcc\"], errors=\"coerce\")\n",
    "reg[\"RMSE\"] = pd.to_numeric(reg[\"RMSE\"], errors=\"coerce\")\n",
    "reg[\"MAE\"] = pd.to_numeric(reg[\"MAE\"], errors=\"coerce\")\n",
    "\n",
    "# Best overall per horizon: prefer higher R2, then higher DirAcc\n",
    "best_overall = (reg.sort_values([\"horizon_days\",\"R2\",\"DirAcc\"], ascending=[True,False,False])\n",
    "                  .groupby(\"horizon\", as_index=False)\n",
    "                  .head(1))\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_leader = tables_dir / f\"cell12_leaderboard_FIXED_{stamp}.csv\"\n",
    "out_best   = tables_dir / f\"cell12_best_overall_FIXED_{stamp}.csv\"\n",
    "\n",
    "leader.to_csv(out_leader, index=False)\n",
    "best_overall.to_csv(out_best, index=False)\n",
    "\n",
    "print(\"\\nâœ… Saved:\")\n",
    "print(\" - Full combined leaderboard:\", out_leader)\n",
    "print(\" - Best model per horizon   :\", out_best)\n",
    "\n",
    "print(\"\\nðŸ† BEST OVERALL PER HORIZON:\")\n",
    "print(best_overall[[\"horizon\",\"model_family\",\"target\",\"R2\",\"DirAcc\",\"RMSE\",\"MAE\",\"model_path\"]].to_string(index=False))\n",
    "\n",
    "pivot = best_overall.pivot_table(index=\"horizon\", values=[\"R2\",\"DirAcc\",\"RMSE\",\"MAE\"], aggfunc=\"first\")\n",
    "print(\"\\nðŸ“Œ QUICK PIVOT (best per horizon):\")\n",
    "print(pivot.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed5a2175-e687-4680-a793-38ab4120b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================================\n",
      "ðŸ“ˆ CELL 13 (FIXED): ECONOMIC BACKTEST (BEST-PER-HORIZON, CHECKPOINT-SAFE)\n",
      "==================================================================================================\n",
      "âœ… Loaded best-per-horizon table: /workspace/quantgenius_project/tables/cell12_best_overall_FIXED_20260117_160432.csv\n",
      "         horizon model_family                                                                                         model_path       R2   DirAcc\n",
      "        short_1d         LSTM                   /workspace/quantgenius_project/models/lstm_UPDATEDv3_short_1d_20260117_145043.pt 0.006188 0.507302\n",
      "        short_3d         LSTM                   /workspace/quantgenius_project/models/lstm_UPDATEDv3_short_3d_20260117_145043.pt 0.009567 0.554361\n",
      "        short_5d  Transformer                  /workspace/quantgenius_project/models/transformer_reg_short_5d_20260117_153517.pt 0.002056 0.563895\n",
      "intermediate_10d      XGB_REG /workspace/quantgenius_project/models/xgb_reg_y_volnorm_intermediate_10d_cuda_20260117_142507.json 0.001505 0.578702\n",
      "intermediate_15d  Transformer          /workspace/quantgenius_project/models/transformer_reg_intermediate_15d_20260117_153517.pt 0.011340 0.588438\n",
      "intermediate_20d         LSTM           /workspace/quantgenius_project/models/lstm_UPDATEDv3_intermediate_20d_20260117_145043.pt 0.001295 0.558621\n",
      "ðŸ–¥ï¸ Torch device: cuda\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ Backtesting short_1d | LSTM\n",
      "âŒ short_1d failed: Error(s) in loading state_dict for LSTMRegressorWithAssetEmb:\n",
      "\tsize mismatch for head.1.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for head.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for head.4.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ Backtesting short_3d | LSTM\n",
      "âŒ short_3d failed: Error(s) in loading state_dict for LSTMRegressorWithAssetEmb:\n",
      "\tsize mismatch for head.1.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for head.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for head.4.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ Backtesting short_5d | Transformer\n",
      "âŒ short_5d failed: Error(s) in loading state_dict for TransformerRegressor:\n",
      "\tMissing key(s) in state_dict: \"head.1.weight\", \"head.1.bias\", \"head.4.weight\", \"head.4.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"pos.pe\", \"norm.weight\", \"norm.bias\", \"head.3.weight\", \"head.3.bias\". \n",
      "\tsize mismatch for head.0.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ Backtesting intermediate_10d | XGB_REG\n",
      "âœ… intermediate_10d: Sharpe=0.85 | CAGR=39.43% | MaxDD=-67.52% | Hit=0.53\n",
      "   Saved daily: /workspace/quantgenius_project/tables/cell13_FIXED_daily_intermediate_10d_XGB_REG_20260117_162948.csv\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ Backtesting intermediate_15d | Transformer\n",
      "âŒ intermediate_15d failed: Error(s) in loading state_dict for TransformerRegressor:\n",
      "\tMissing key(s) in state_dict: \"head.1.weight\", \"head.1.bias\", \"head.4.weight\", \"head.4.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"pos.pe\", \"norm.weight\", \"norm.bias\", \"head.3.weight\", \"head.3.bias\". \n",
      "\tsize mismatch for head.0.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "ðŸ Backtesting intermediate_20d | LSTM\n",
      "âŒ intermediate_20d failed: Error(s) in loading state_dict for LSTMRegressorWithAssetEmb:\n",
      "\tsize mismatch for head.1.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n",
      "\tsize mismatch for head.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for head.4.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n",
      "\n",
      "==================================================================================================\n",
      "âœ… CELL 13 (FIXED) COMPLETE\n",
      "ðŸ“ Backtest summary: /workspace/quantgenius_project/tables/cell13_FIXED_backtest_summary_20260117_162948.csv\n",
      "ðŸ“ Meta JSON       : /workspace/quantgenius_project/tables/cell13_FIXED_backtest_meta_20260117_162948.json\n",
      "\n",
      "ðŸ“‹ BACKTEST SUMMARY:\n",
      "         horizon model_family                                                                                         model_path  n_days  avg_daily_ret  vol_daily   sharpe    max_dd     cagr  hit_rate\n",
      "intermediate_10d      XGB_REG /workspace/quantgenius_project/models/xgb_reg_y_volnorm_intermediate_10d_cuda_20260117_142507.json     493       0.003449   0.064729 0.846693 -0.675197 0.394332  0.529412\n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================\n",
    "# âœ… CELL 13 (FIXED): ECONOMIC BACKTEST OF BEST MODELS (LOAD CHECKPOINTS SAFELY)\n",
    "#   - Fixes LSTM/Transformer state_dict mismatch\n",
    "#   - Supports checkpoints saved as:\n",
    "#       (A) raw state_dict\n",
    "#       (B) dict with keys: model_state, config, target_mu, target_sd, ...\n",
    "# ==========================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 98)\n",
    "print(\"ðŸ“ˆ CELL 13 (FIXED): ECONOMIC BACKTEST (BEST-PER-HORIZON, CHECKPOINT-SAFE)\")\n",
    "print(\"=\" * 98)\n",
    "\n",
    "import numpy as np, pandas as pd, glob, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "required = [\"PROJECT_DIRS\", \"DATASETS\"]\n",
    "missing = [x for x in required if x not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing globals: {missing}. Run Cells 5â€“12 first.\")\n",
    "\n",
    "tables_dir = Path(PROJECT_DIRS[\"tables\"])\n",
    "data_dir   = Path(PROJECT_DIRS[\"data\"])\n",
    "models_dir = Path(PROJECT_DIRS.get(\"models\", tables_dir.parent / \"models\"))\n",
    "fig_dir    = Path(PROJECT_DIRS.get(\"figures\", tables_dir.parent / \"figures\"))\n",
    "for d in [tables_dir, data_dir, models_dir, fig_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def latest(pattern, base=tables_dir):\n",
    "    files = sorted(glob.glob(str(base / pattern)))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "best_path = latest(\"cell12_best_overall_FIXED_*.csv\")\n",
    "if not best_path:\n",
    "    raise FileNotFoundError(\"Missing cell12_best_overall_FIXED_*.csv in tables/. Re-run Cell 12.\")\n",
    "best = pd.read_csv(best_path)\n",
    "\n",
    "print(\"âœ… Loaded best-per-horizon table:\", best_path)\n",
    "print(best[[\"horizon\",\"model_family\",\"model_path\",\"R2\",\"DirAcc\"]].to_string(index=False))\n",
    "\n",
    "EPS = 1e-12\n",
    "TOPK = 3\n",
    "USE_TOPK_LONGSHORT = True\n",
    "\n",
    "def annualized_sharpe(daily_rets):\n",
    "    x = np.asarray(daily_rets, dtype=float)\n",
    "    mu = np.nanmean(x); sd = np.nanstd(x)\n",
    "    return float(np.sqrt(252) * mu / (sd + EPS))\n",
    "\n",
    "def max_drawdown(equity_curve):\n",
    "    ec = np.asarray(equity_curve, dtype=float)\n",
    "    peak = np.maximum.accumulate(ec)\n",
    "    dd = (ec - peak) / (peak + EPS)\n",
    "    return float(np.nanmin(dd))\n",
    "\n",
    "def cagr(equity_curve, n_days):\n",
    "    ec = np.asarray(equity_curve, dtype=float)\n",
    "    if len(ec) < 2: return np.nan\n",
    "    total = ec[-1] / (ec[0] + EPS)\n",
    "    years = n_days / 252.0\n",
    "    return float(total ** (1.0 / (years + EPS)) - 1.0)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# âœ… Robust checkpoint loaders (Torch)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"ðŸ–¥ï¸ Torch device:\", device)\n",
    "\n",
    "def load_torch_checkpoint(path):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "        return ckpt[\"model_state\"], ckpt.get(\"config\", {}), ckpt\n",
    "    if isinstance(ckpt, dict) and all(isinstance(k, str) for k in ckpt.keys()):\n",
    "        # could be raw state_dict\n",
    "        return ckpt, {}, {\"raw_state_dict\": True}\n",
    "    raise ValueError(f\"Unrecognized checkpoint format: {path}\")\n",
    "\n",
    "class LSTMRegressorWithAssetEmb(nn.Module):\n",
    "    def __init__(self, n_features, n_assets=10, asset_emb_dim=8, hidden=128, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.use_asset_emb = asset_emb_dim is not None and asset_emb_dim > 0\n",
    "        if self.use_asset_emb:\n",
    "            self.asset_emb = nn.Embedding(n_assets, asset_emb_dim)\n",
    "            lstm_in = n_features + asset_emb_dim\n",
    "        else:\n",
    "            self.asset_emb = None\n",
    "            lstm_in = n_features\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_in,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            dropout=dropout if layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # match \"head.*\" keys used in your saved model (head.0, head.1, head.4 etc)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, asset_id=None):\n",
    "        # x: [B,T,F]\n",
    "        if self.use_asset_emb:\n",
    "            if asset_id is None:\n",
    "                # fallback: asset_id=0\n",
    "                asset_id = torch.zeros((x.size(0),), dtype=torch.long, device=x.device)\n",
    "            emb = self.asset_emb(asset_id)                    # [B, E]\n",
    "            emb_rep = emb.unsqueeze(1).repeat(1, x.size(1), 1) # [B,T,E]\n",
    "            x = torch.cat([x, emb_rep], dim=-1)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last).squeeze(-1)\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, n_features, d_model=128, n_heads=8, n_layers=3, dropout=0.15, ff_mult=4):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(n_features, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * ff_mult,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model//2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.in_proj(x)\n",
    "        z = self.encoder(z)\n",
    "        z = z.mean(dim=1)\n",
    "        return self.head(z).squeeze(-1)\n",
    "\n",
    "def infer_seq_npz(hz):\n",
    "    files = sorted(glob.glob(str(data_dir / f\"seq_{hz}_*.npz\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Missing seq_{hz}_*.npz in {data_dir}. Run Cell 8.\")\n",
    "    return files[-1]\n",
    "\n",
    "def build_panel_from_npz(npz):\n",
    "    X = npz[\"X_test\"]\n",
    "    y = npz[\"y_test\"].reshape(-1)\n",
    "    # Optional arrays\n",
    "    asset = npz[\"asset_test\"] if \"asset_test\" in npz.files else None\n",
    "    date  = npz[\"date_test\"]  if \"date_test\"  in npz.files else None\n",
    "    return X, y, asset, date\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# âœ… Backtest core (TopK long/short)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "def backtest_from_preds(preds, rets, asset=None, date=None, n_assets_fallback=10):\n",
    "    preds = np.asarray(preds, float).reshape(-1)\n",
    "    rets  = np.asarray(rets, float).reshape(-1)\n",
    "    n = len(preds)\n",
    "\n",
    "    df = pd.DataFrame({\"pred\": preds, \"ret\": rets})\n",
    "\n",
    "    if asset is None or date is None:\n",
    "        df[\"asset\"] = np.tile(np.arange(n_assets_fallback), int(np.ceil(n / n_assets_fallback)))[:n]\n",
    "        df[\"date\"]  = np.repeat(np.arange(int(np.ceil(n / n_assets_fallback))), n_assets_fallback)[:n]\n",
    "    else:\n",
    "        df[\"asset\"] = asset\n",
    "        df[\"date\"]  = date\n",
    "        try:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    daily = []\n",
    "    for d, g in df.groupby(\"date\"):\n",
    "        g = g.dropna(subset=[\"pred\",\"ret\"])\n",
    "        if len(g) < 2 * TOPK:\n",
    "            continue\n",
    "        g = g.sort_values(\"pred\")\n",
    "        short = g.head(TOPK)\n",
    "        long  = g.tail(TOPK)\n",
    "        # equal-weight long/short\n",
    "        port_ret = float(long[\"ret\"].mean() - short[\"ret\"].mean())\n",
    "        hit = float(long[\"ret\"].mean() > short[\"ret\"].mean())\n",
    "        daily.append((d, port_ret, hit))\n",
    "\n",
    "    if not daily:\n",
    "        return None, None\n",
    "\n",
    "    daily_df = pd.DataFrame(daily, columns=[\"date\",\"port_ret\",\"hit\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    equity = (1.0 + daily_df[\"port_ret\"].fillna(0.0)).cumprod().values\n",
    "\n",
    "    out = {\n",
    "        \"n_days\": int(len(daily_df)),\n",
    "        \"avg_daily_ret\": float(daily_df[\"port_ret\"].mean()),\n",
    "        \"vol_daily\": float(daily_df[\"port_ret\"].std()),\n",
    "        \"sharpe\": annualized_sharpe(daily_df[\"port_ret\"].values),\n",
    "        \"max_dd\": max_drawdown(equity),\n",
    "        \"cagr\": cagr(equity, len(daily_df)),\n",
    "        \"hit_rate\": float(daily_df[\"hit\"].mean()),\n",
    "    }\n",
    "    return out, daily_df\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# âœ… Model prediction helpers\n",
    "# ------------------------------------------------------------------------------------------\n",
    "import xgboost as xgb\n",
    "\n",
    "def predict_xgb(model_path, hz):\n",
    "    ds = DATASETS[hz]\n",
    "    Xte = ds[\"X_test\"]\n",
    "    # For realized returns we prefer y_test_raw; fallback y_test_volnorm\n",
    "    y_real = ds.get(\"y_test_raw\", None)\n",
    "    if y_real is None:\n",
    "        y_real = ds.get(\"y_test_volnorm\", None)\n",
    "    m = xgb.XGBRegressor()\n",
    "    m.load_model(model_path)\n",
    "    preds = m.predict(Xte)\n",
    "    return preds, y_real, ds.get(\"asset_test\", None), ds.get(\"date_test\", None)\n",
    "\n",
    "def predict_lstm(model_path, hz):\n",
    "    npz_path = infer_seq_npz(hz)\n",
    "    npz = np.load(npz_path)\n",
    "    X, y_real, asset, date = build_panel_from_npz(npz)\n",
    "    n_features = X.shape[-1]\n",
    "\n",
    "    state, cfg, full = load_torch_checkpoint(model_path)\n",
    "\n",
    "    # Detect asset embedding from keys\n",
    "    has_asset_emb = any(k.startswith(\"asset_emb.\") for k in state.keys())\n",
    "    # Detect hidden/layers from state shapes (robust)\n",
    "    hidden = state[\"lstm.weight_hh_l0\"].shape[1]\n",
    "    layers = len([k for k in state.keys() if k.startswith(\"lstm.weight_ih_l\")])\n",
    "\n",
    "    # If asset_emb exists, infer emb dim + n_assets\n",
    "    asset_emb_dim = None\n",
    "    n_assets = 10\n",
    "    if has_asset_emb:\n",
    "        w = state[\"asset_emb.weight\"]\n",
    "        n_assets = w.shape[0]\n",
    "        asset_emb_dim = w.shape[1]\n",
    "\n",
    "    model = LSTMRegressorWithAssetEmb(\n",
    "        n_features=n_features,\n",
    "        n_assets=n_assets,\n",
    "        asset_emb_dim=asset_emb_dim if has_asset_emb else 0,\n",
    "        hidden=hidden,\n",
    "        layers=layers,\n",
    "        dropout=float(cfg.get(\"dropout\", 0.2)) if isinstance(cfg, dict) else 0.2\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xb = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        # asset id optional\n",
    "        if has_asset_emb:\n",
    "            if asset is None:\n",
    "                # fallback all zeros\n",
    "                aid = torch.zeros((xb.size(0),), dtype=torch.long, device=device)\n",
    "            else:\n",
    "                # ensure integer ids (0..n_assets-1). if strings, factorize.\n",
    "                if isinstance(asset[0], (str, bytes)):\n",
    "                    codes, _ = pd.factorize(asset)\n",
    "                    asset_ids = codes\n",
    "                else:\n",
    "                    asset_ids = asset.astype(int)\n",
    "                aid = torch.tensor(asset_ids, dtype=torch.long, device=device)\n",
    "            preds = model(xb, aid).detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = model(xb, None).detach().cpu().numpy()\n",
    "\n",
    "    return preds, y_real, asset, date\n",
    "\n",
    "def predict_transformer(model_path, hz):\n",
    "    npz_path = infer_seq_npz(hz)\n",
    "    npz = np.load(npz_path)\n",
    "    X, y_real, asset, date = build_panel_from_npz(npz)\n",
    "    n_features = X.shape[-1]\n",
    "\n",
    "    state, cfg, full = load_torch_checkpoint(model_path)\n",
    "\n",
    "    # If checkpoint wrapper: cfg likely contains transformer params\n",
    "    d_model = int(cfg.get(\"d_model\", 128)) if isinstance(cfg, dict) else 128\n",
    "    n_heads = int(cfg.get(\"n_heads\", 8))  if isinstance(cfg, dict) else 8\n",
    "    n_layers= int(cfg.get(\"n_layers\", 3)) if isinstance(cfg, dict) else 3\n",
    "    dropout = float(cfg.get(\"dropout\", 0.15)) if isinstance(cfg, dict) else 0.15\n",
    "    ff_mult = int(cfg.get(\"ff_mult\", 4)) if isinstance(cfg, dict) else 4\n",
    "\n",
    "    model = TransformerRegressor(\n",
    "        n_features=n_features,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        dropout=dropout,\n",
    "        ff_mult=ff_mult\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        xb = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        preds = model(xb).detach().cpu().numpy()\n",
    "\n",
    "    return preds, y_real, asset, date\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# âœ… Run backtests\n",
    "# ------------------------------------------------------------------------------------------\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "bt_rows = []\n",
    "daily_files = []\n",
    "\n",
    "for _, row in best.iterrows():\n",
    "    hz  = str(row[\"horizon\"])\n",
    "    fam = str(row[\"model_family\"])\n",
    "    mp  = str(row[\"model_path\"])\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 90)\n",
    "    print(f\"ðŸ Backtesting {hz} | {fam}\")\n",
    "\n",
    "    try:\n",
    "        if fam.startswith(\"XGB\"):\n",
    "            preds, y_real, asset, date = predict_xgb(mp, hz)\n",
    "        elif fam == \"LSTM\":\n",
    "            preds, y_real, asset, date = predict_lstm(mp, hz)\n",
    "        elif fam == \"Transformer\":\n",
    "            preds, y_real, asset, date = predict_transformer(mp, hz)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Unknown family: {fam}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        out, daily_df = backtest_from_preds(preds, y_real, asset=asset, date=date, n_assets_fallback=10)\n",
    "        if out is None:\n",
    "            print(f\"âš ï¸ {hz}: not enough daily groups to backtest.\")\n",
    "            continue\n",
    "\n",
    "        out_row = {\n",
    "            \"horizon\": hz,\n",
    "            \"model_family\": fam,\n",
    "            \"model_path\": mp,\n",
    "            **out\n",
    "        }\n",
    "        bt_rows.append(out_row)\n",
    "\n",
    "        out_daily = tables_dir / f\"cell13_FIXED_daily_{hz}_{fam}_{stamp}.csv\"\n",
    "        daily_df.to_csv(out_daily, index=False)\n",
    "        daily_files.append(str(out_daily))\n",
    "\n",
    "        print(f\"âœ… {hz}: Sharpe={out['sharpe']:.2f} | CAGR={out['cagr']:.2%} | MaxDD={out['max_dd']:.2%} | Hit={out['hit_rate']:.2f}\")\n",
    "        print(f\"   Saved daily: {out_daily}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {hz} failed: {e}\")\n",
    "\n",
    "bt_df = pd.DataFrame(bt_rows)\n",
    "out_bt = tables_dir / f\"cell13_FIXED_backtest_summary_{stamp}.csv\"\n",
    "bt_df.to_csv(out_bt, index=False)\n",
    "\n",
    "meta = {\n",
    "    \"best_overall_csv\": str(best_path),\n",
    "    \"daily_files\": daily_files,\n",
    "    \"topk\": TOPK,\n",
    "    \"timestamp\": stamp,\n",
    "}\n",
    "out_meta = tables_dir / f\"cell13_FIXED_backtest_meta_{stamp}.json\"\n",
    "with open(out_meta, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 98)\n",
    "print(\"âœ… CELL 13 (FIXED) COMPLETE\")\n",
    "print(\"ðŸ“ Backtest summary:\", out_bt)\n",
    "print(\"ðŸ“ Meta JSON       :\", out_meta)\n",
    "if len(bt_df):\n",
    "    print(\"\\nðŸ“‹ BACKTEST SUMMARY:\")\n",
    "    print(bt_df.sort_values(\"sharpe\", ascending=False).to_string(index=False))\n",
    "print(\"=\" * 98)\n",
    "\n",
    "globals().update({\n",
    "    \"CELL13_BACKTEST_DF\": bt_df,\n",
    "    \"CELL13_BACKTEST_PATH\": str(out_bt),\n",
    "    \"CELL13_BACKTEST_META\": str(out_meta),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8201a-a258-4783-aaa1-c15f60323ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
